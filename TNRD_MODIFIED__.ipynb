{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "5zuWCQAXLJW5",
        "outputId": "4fe6dd6a-eaea-402c-ece3-fdba70edc404"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-colab-shell in /usr/local/lib/python3.10/dist-packages (0.2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<!--https://github.com/singhsidhukuldeep/Google-Colab-Shell/-->\n",
              "<!--Using JS/CSS from official sources with backups in-case :)-->\n",
              "<div id=colab_shell></div>\n",
              "<script src=\"https://code.jquery.com/jquery-latest.js\"></script>\n",
              "<script src=\"https://cdn.jsdelivr.net/npm/jquery.terminal/js/jquery.terminal.min.js\"></script>\n",
              "<link href=\"https://cdn.jsdelivr.net/npm/jquery.terminal/css/jquery.terminal.min.css\" rel=\"stylesheet\"/>\n",
              "<script>\n",
              "   $('#colab_shell').terminal(async function(command) {\n",
              "       if (command !== '') {\n",
              "           try {\n",
              "               let res = await google.colab.kernel.invokeFunction('shell', [command])\n",
              "               let out = res.data['application/json'][0]\n",
              "               this.echo(new String(out))\n",
              "           } catch(e) {\n",
              "               this.error(new String(e));\n",
              "           }\n",
              "       } else {\n",
              "           this.echo(\n",
              "             //   '>>Empty Command<<\\n'+\n",
              "             //   'If you can afford use Google Colab Pro'\n",
              "               );\n",
              "       }\n",
              "   }, {\n",
              "       greetings: 'Welcome to Google Colab Shell\\n'+\n",
              "         'If you can afford, please use Google Colab Pro ( https://colab.research.google.com/signup )\\n'+\n",
              "         '⭐ STAR the repo ⭐\\n https://github.com/singhsidhukuldeep/Google-Colab-Shell\\n\\n',\n",
              "       name: 'colab_shell',\n",
              "       height: 400,\n",
              "       prompt: '█$ colab>>\\t'\n",
              "   });\n",
              "</script>\n"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "!pip install google-colab-shell\n",
        "\n",
        "from google_colab_shell import getshell\n",
        "\n",
        "getshell()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**data->dataset.py(multiplicative gamma noise)**\n",
        "In the provided DatasetNoise class, I can add Multiplicative gamma noise to the input images in the __getitem__ method."
      ],
      "metadata": {
        "id": "M2wkCmXH2Ge2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "class DatasetNoise(torch.utils.data.dataset.Dataset):\n",
        "    def __init__(self, root='/content/denoising/data/FoETrainingSets180/', noise_sigma=50., training=True, crop_size=60, blind_denoising=False, gray_scale=False, max_size=None):\n",
        "        self.root = root\n",
        "        self.noise_sigma = noise_sigma\n",
        "        self.training = training\n",
        "        self.crop_size = crop_size\n",
        "        self.blind_denoising = blind_denoising\n",
        "        self.gray_scale = gray_scale\n",
        "        self.max_size = max_size\n",
        "\n",
        "        self._init()\n",
        "\n",
        "    def _init(self):\n",
        "        # data paths\n",
        "        targets = glob.glob(os.path.join(self.root, '*.*'))[:self.max_size]\n",
        "        self.paths = {'target' : targets}\n",
        "\n",
        "        # transforms\n",
        "        t_list = [transforms.ToTensor()]\n",
        "        self.image_transform = transforms.Compose(t_list)\n",
        "\n",
        "    def _get_augment_params(self, size):\n",
        "        random.seed(random.randint(0, 12345))\n",
        "\n",
        "        # position\n",
        "        w_size, h_size = size\n",
        "        x = random.randint(0, max(0, w_size - self.crop_size))\n",
        "        y = random.randint(0, max(0, h_size - self.crop_size))\n",
        "\n",
        "        # flip\n",
        "        flip = random.random() > 0.5\n",
        "        return {'crop_pos': (x, y), 'flip': flip}\n",
        "\n",
        "    def _augment(self, image, aug_params):\n",
        "        x, y = aug_params['crop_pos']\n",
        "        image = image.crop((x, y, x + self.crop_size, y + self.crop_size))\n",
        "        if aug_params['flip']:\n",
        "            image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "        return image\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # target image\n",
        "        if self.gray_scale:\n",
        "            target = Image.open(self.paths['target'][index]).convert('L')\n",
        "        else:\n",
        "            target = Image.open(self.paths['target'][index]).convert('RGB')\n",
        "\n",
        "        # transform\n",
        "        if self.training:\n",
        "            aug_params = self._get_augment_params(target.size)\n",
        "            target = self._augment(target, aug_params)\n",
        "        target = self.image_transform(target) * 255\n",
        "\n",
        "        # add multiplicative gamma noise\n",
        "        if self.blind_denoising:\n",
        "            noise_sigma = random.randint(0, self.noise_sigma)\n",
        "        else:\n",
        "            noise_sigma = self.noise_sigma\n",
        "\n",
        "        shape = target.shape\n",
        "        gamma_shape = shape  # the shape of the gamma noise should match the image\n",
        "        gamma_noise = torch.from_numpy(np.random.gamma(shape=1.0, scale=noise_sigma / 255.0, size=gamma_shape)).float()\n",
        "\n",
        "        input = target * gamma_noise\n",
        "\n",
        "        return {'input': input, 'target': target, 'path': self.paths['target'][index]}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths['target'])\n",
        "\n",
        "# Usage\n",
        "# dataset = DatasetNoise(root='path_to_your_dataset', noise_sigma=50., training=True, crop_size=60, blind_denoising=False, gray_scale=False, max_size=None)\n",
        "# data_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "3n3zjmxV2EYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Epoch 2\n",
        "\n",
        "Epoch 3\n",
        "\n",
        "Epoch 4\n",
        "\n",
        "Epoch 5\n",
        "\n",
        "Epoch 6\n",
        "\n",
        "Epoch 7\n",
        "\n",
        "Epoch 8\n",
        "\n",
        "Epoch 9\n",
        "\n",
        "Epoch 10\n",
        "Iteration 101, G: 4887.37382, G_recon: 4887.373817, tnrd_loss: 0.003334,\n",
        "Evaluation: 16.032\n",
        "\n",
        "Epoch 11\n",
        "\n",
        "Epoch 12\n",
        "\n",
        "Epoch 13\n",
        "\n",
        "Epoch 14\n",
        "\n",
        "Epoch 15\n",
        "\n",
        "Epoch 16\n",
        "\n",
        "Epoch 17\n",
        "\n",
        "Epoch 18\n",
        "\n",
        "Epoch 19\n",
        "Iteration 201, G: 1611.86506, G_recon: 1611.865063, tnrd_loss: 0.002062,\n",
        "\n",
        "Epoch 20\n",
        "Evaluation: 17.420\n",
        "Best PSNR Score: 17.42"
      ],
      "metadata": {
        "id": "bUU0m14H_b5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**additive_gamma(data->dataset.py)**"
      ],
      "metadata": {
        "id": "Jicyw3Sb3-Lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "class DatasetNoise(torch.utils.data.dataset.Dataset):\n",
        "    def __init__(self, root='/content/denoising/data/FoETrainingSets180/', noise_sigma=50., training=True, crop_size=60, blind_denoising=False, gray_scale=False, max_size=None):\n",
        "        self.root = root\n",
        "        self.noise_sigma = noise_sigma\n",
        "        self.training = training\n",
        "        self.crop_size = crop_size\n",
        "        self.blind_denoising = blind_denoising\n",
        "        self.gray_scale = gray_scale\n",
        "        self.max_size = max_size\n",
        "\n",
        "        self._init()\n",
        "\n",
        "    def _init(self):\n",
        "        # data paths\n",
        "        targets = glob.glob(os.path.join(self.root, '*.*'))[:self.max_size]\n",
        "        self.paths = {'target': targets}\n",
        "\n",
        "        # transforms\n",
        "        t_list = [transforms.ToTensor()]\n",
        "        self.image_transform = transforms.Compose(t_list)\n",
        "\n",
        "    def _get_augment_params(self, size):\n",
        "        random.seed(random.randint(0, 12345))\n",
        "\n",
        "        # position\n",
        "        w_size, h_size = size\n",
        "        x = random.randint(0, max(0, w_size - self.crop_size))\n",
        "        y = random.randint(0, max(0, h_size - self.crop_size))\n",
        "\n",
        "        # flip\n",
        "        flip = random.random() > 0.5\n",
        "        return {'crop_pos': (x, y), 'flip': flip}\n",
        "\n",
        "    def _augment(self, image, aug_params):\n",
        "        x, y = aug_params['crop_pos']\n",
        "        image = image.crop((x, y, x + self.crop_size, y + self.crop_size))\n",
        "        if aug_params['flip']:\n",
        "            image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "        return image\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # target image\n",
        "        if self.gray_scale:\n",
        "            target = Image.open(self.paths['target'][index]).convert('L')\n",
        "        else:\n",
        "            target = Image.open(self.paths['target'][index]).convert('RGB')\n",
        "\n",
        "        # transform\n",
        "        if self.training:\n",
        "            aug_params = self._get_augment_params(target.size)\n",
        "            target = self._augment(target, aug_params)\n",
        "        target = self.image_transform(target) * 255\n",
        "\n",
        "        # add additive gamma noise\n",
        "        if self.blind_denoising:\n",
        "            noise_sigma = random.uniform(0, self.noise_sigma)\n",
        "        else:\n",
        "            noise_sigma = self.noise_sigma\n",
        "\n",
        "        shape = target.shape\n",
        "        additive_gamma_noise = torch.from_numpy(np.random.gamma(shape=1.0, scale=noise_sigma, size=shape)).float()\n",
        "        input = target + additive_gamma_noise\n",
        "\n",
        "        return {'input': input, 'target': target, 'path': self.paths['target'][index]}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths['target'])\n",
        "\n",
        "# Usage\n",
        "# dataset = DatasetNoise(root='path_to_your_dataset', noise_sigma=50., training=True, crop_size=60, blind_denoising=False, gray_scale=False, max_size=None)\n",
        "# data_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "--FaRwQU37XL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "output:\n",
        "Epoch 2\n",
        "\n",
        "Epoch 3\n",
        "\n",
        "Epoch 4\n",
        "\n",
        "Epoch 5\n",
        "\n",
        "Epoch 6\n",
        "\n",
        "Epoch 7\n",
        "\n",
        "Epoch 8\n",
        "\n",
        "Epoch 9\n",
        "\n",
        "Epoch 10\n",
        "Iteration 101, G: 2574.21986, G_recon: 2574.219865, tnrd_loss: 0.133180,\n",
        "Evaluation: 15.482\n",
        "\n",
        "Epoch 11\n",
        "\n",
        "Epoch 12\n",
        "\n",
        "Epoch 13\n",
        "\n",
        "Epoch 14\n",
        "\n",
        "Epoch 15\n",
        "\n",
        "Epoch 16\n",
        "\n",
        "Epoch 17\n",
        "\n",
        "Epoch 18\n",
        "\n",
        "Epoch 19\n",
        "Iteration 201, G: 1775.26349, G_recon: 1775.263491, tnrd_loss: 1.594507,\n",
        "\n",
        "Epoch 20\n",
        "Evaluation: 16.354\n",
        "Best PSNR Score: 16.35"
      ],
      "metadata": {
        "id": "3_QL78h__CNI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gaussion_noise(data->dataset.py)**"
      ],
      "metadata": {
        "id": "YY5YaiiP5FzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "class DatasetNoise(torch.utils.data.dataset.Dataset):\n",
        "    def __init__(self, root='/content/denoising/data/FoETrainingSets180/', noise_sigma=50., training=True, crop_size=60, blind_denoising=False, gray_scale=False, max_size=None):\n",
        "        self.root = root\n",
        "        self.noise_sigma = noise_sigma\n",
        "        self.training = training\n",
        "        self.crop_size = crop_size\n",
        "        self.blind_denoising = blind_denoising\n",
        "        self.gray_scale = gray_scale\n",
        "        self.max_size = max_size\n",
        "\n",
        "        self._init()\n",
        "\n",
        "    def _init(self):\n",
        "        # data paths\n",
        "        targets = glob.glob(os.path.join(self.root, '*.*'))[:self.max_size]\n",
        "        self.paths = {'target': targets}\n",
        "\n",
        "        # transforms\n",
        "        t_list = [transforms.ToTensor()]\n",
        "        self.image_transform = transforms.Compose(t_list)\n",
        "\n",
        "    def _get_augment_params(self, size):\n",
        "        random.seed(random.randint(0, 12345))\n",
        "\n",
        "        # position\n",
        "        w_size, h_size = size\n",
        "        x = random.randint(0, max(0, w_size - self.crop_size))\n",
        "        y = random.randint(0, max(0, h_size - self.crop_size))\n",
        "\n",
        "        # flip\n",
        "        flip = random.random() > 0.5\n",
        "        return {'crop_pos': (x, y), 'flip': flip}\n",
        "\n",
        "    def _augment(self, image, aug_params):\n",
        "        x, y = aug_params['crop_pos']\n",
        "        image = image.crop((x, y, x + self.crop_size, y + self.crop_size))\n",
        "        if aug_params['flip']:\n",
        "            image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "        return image\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # target image\n",
        "        if self.gray_scale:\n",
        "            target = Image.open(self.paths['target'][index]).convert('L')\n",
        "        else:\n",
        "            target = Image.open(self.paths['target'][index]).convert('RGB')\n",
        "\n",
        "        # transform\n",
        "        if self.training:\n",
        "            aug_params = self._get_augment_params(target.size)\n",
        "            target = self._augment(target, aug_params)\n",
        "        target = self.image_transform(target) * 255\n",
        "\n",
        "        # add Gaussian noise\n",
        "        if self.blind_denoising:\n",
        "            noise_sigma = random.uniform(0, self.noise_sigma)\n",
        "        else:\n",
        "            noise_sigma = self.noise_sigma\n",
        "\n",
        "        gaussian_noise = noise_sigma * torch.randn_like(target)\n",
        "        input = target + gaussian_noise\n",
        "\n",
        "        return {'input': input, 'target': target, 'path': self.paths['target'][index]}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths['target'])\n",
        "\n",
        "# Usage\n",
        "# dataset = DatasetNoise(root='path_to_your_dataset', noise_sigma=50., training=True, crop_size=60, blind_denoising=False, gray_scale=False, max_size=None)\n",
        "# data_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "RF6qGLKa5ENo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "output:\n",
        "Epoch 2\n",
        "\n",
        "Epoch 3\n",
        "\n",
        "Epoch 4\n",
        "\n",
        "Epoch 5\n",
        "\n",
        "Epoch 6\n",
        "\n",
        "Epoch 7\n",
        "\n",
        "Epoch 8\n",
        "\n",
        "Epoch 9\n",
        "\n",
        "Epoch 10\n",
        "Iteration 101, G: 1986.47201, G_recon: 1986.472006, tnrd_loss: 7.114801,\n",
        "Evaluation: 16.220\n",
        "\n",
        "Epoch 11\n",
        "\n",
        "Epoch 12\n",
        "\n",
        "Epoch 13\n",
        "\n",
        "Epoch 14\n",
        "\n",
        "Epoch 15\n",
        "\n",
        "Epoch 16\n",
        "\n",
        "Epoch 17\n",
        "\n",
        "Epoch 18\n",
        "\n",
        "Epoch 19\n",
        "Iteration 201, G: 1109.34377, G_recon: 1109.343768, tnrd_loss: 883.907410,\n",
        "\n",
        "Epoch 20\n",
        "Evaluation: 21.615\n",
        "Best PSNR Score: 21.61"
      ],
      "metadata": {
        "id": "wKr94Wzg-ypf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Add Multiplicative_Gamma_Noise:datasets.py(In the provided DatasetNoise class, you can add Multiplicative_Gamma noise to the input images in the __getitem__ method)                               activations.py(This modification allows you to incorporate Multiplicative_Gamma noise into the input before applying the RBF transformation within your neural network model.)**"
      ],
      "metadata": {
        "id": "grp0QJRk8hK5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "data ->> datasets.py"
      ],
      "metadata": {
        "id": "qPCrWYae97Uh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "class DatasetNoise(torch.utils.data.dataset.Dataset):\n",
        "    def __init__(self, root='/content/denoising/data/FoETrainingSets180/', noise_sigma=50., training=True, crop_size=60, blind_denoising=False, gray_scale=False, max_size=None):\n",
        "        self.root = root\n",
        "        self.noise_sigma = noise_sigma\n",
        "        self.training = training\n",
        "        self.crop_size = crop_size\n",
        "        self.blind_denoising = blind_denoising\n",
        "        self.gray_scale = gray_scale\n",
        "        self.max_size = max_size\n",
        "\n",
        "        self._init()\n",
        "\n",
        "    def _init(self):\n",
        "        # data paths\n",
        "        targets = glob.glob(os.path.join(self.root, '*.*'))[:self.max_size]\n",
        "        self.paths = {'target' : targets}\n",
        "\n",
        "        # transforms\n",
        "        t_list = [transforms.ToTensor()]\n",
        "        self.image_transform = transforms.Compose(t_list)\n",
        "\n",
        "    def _get_augment_params(self, size):\n",
        "        random.seed(random.randint(0, 12345))\n",
        "\n",
        "        # position\n",
        "        w_size, h_size = size\n",
        "        x = random.randint(0, max(0, w_size - self.crop_size))\n",
        "        y = random.randint(0, max(0, h_size - self.crop_size))\n",
        "\n",
        "        # flip\n",
        "        flip = random.random() > 0.5\n",
        "        return {'crop_pos': (x, y), 'flip': flip}\n",
        "\n",
        "    def _augment(self, image, aug_params):\n",
        "        x, y = aug_params['crop_pos']\n",
        "        image = image.crop((x, y, x + self.crop_size, y + self.crop_size))\n",
        "        if aug_params['flip']:\n",
        "            image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "        return image\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # target image\n",
        "        if self.gray_scale:\n",
        "            target = Image.open(self.paths['target'][index]).convert('L')\n",
        "        else:\n",
        "            target = Image.open(self.paths['target'][index]).convert('RGB')\n",
        "\n",
        "        # transform\n",
        "        if self.training:\n",
        "            aug_params = self._get_augment_params(target.size)\n",
        "            target = self._augment(target, aug_params)\n",
        "        target = self.image_transform(target) * 255\n",
        "\n",
        "        # add multiplicative gamma noise\n",
        "        if self.blind_denoising:\n",
        "            noise_sigma = random.randint(0, self.noise_sigma)\n",
        "        else:\n",
        "            noise_sigma = self.noise_sigma\n",
        "\n",
        "        shape = target.shape\n",
        "        gamma_shape = shape  # the shape of the gamma noise should match the image\n",
        "        gamma_noise = torch.from_numpy(np.random.gamma(shape=1.0, scale=noise_sigma / 255.0, size=gamma_shape)).float()\n",
        "\n",
        "        input = target * gamma_noise\n",
        "\n",
        "        return {'input': input, 'target': target, 'path': self.paths['target'][index]}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths['target'])\n",
        "\n",
        "# Usage\n",
        "# dataset = DatasetNoise(root='path_to_your_dataset', noise_sigma=50., training=True, crop_size=60, blind_denoising=False, gray_scale=False, max_size=None)\n",
        "# data_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "k96VeTbg8gw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model -> activations.py"
      ],
      "metadata": {
        "id": "tLBnBzwi-FRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import numpy as np\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# RBF Layer\n",
        "\n",
        "# RBF Layer\n",
        "class RBF(nn.Module):\n",
        "    \"\"\"\n",
        "    Transforms incoming data using a given radial basis function:\n",
        "    u_{i} = rbf(||x - c_{i}|| / s_{i})\n",
        "    Arguments:\n",
        "        in_features: size of each input sample\n",
        "        out_features: size of each output sample\n",
        "    Shape:\n",
        "        - Input: (N, in_features) where N is an arbitrary batch size\n",
        "        - Output: (N, out_features) where N is an arbitrary batch size\n",
        "    Attributes:\n",
        "        centers: the learnable centers of shape (out_features, in_features).\n",
        "            The values are initialised from a standard normal distribution.\n",
        "            Normalising inputs to have mean 0 and standard deviation 1 is\n",
        "            recommended.\n",
        "\n",
        "        sigmas: the learnable scaling factors of shape (out_features).\n",
        "            The values are initialised as ones.\n",
        "\n",
        "        basis_func: the radial basis function used to transform the scaled\n",
        "            distances.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_func, num_filters, basis_func):\n",
        "        super(RBF, self).__init__()\n",
        "        self.num_func = num_func\n",
        "        self.register_buffer('centers', torch.tensor(np.linspace(-310, 310, num_func)).float())\n",
        "        self.num_filters = num_filters\n",
        "        self.weight = nn.Parameter(torch.Tensor(num_func, 1, num_filters))\n",
        "        self.gamma = 10\n",
        "        self.basis_func = basis_func\n",
        "        self.int_basis_func = erf_func\n",
        "\n",
        "    def forward(self, input, shape_param=2.0, scale_param=1.0):\n",
        "        # Generate and apply multiplicative gamma noise\n",
        "        noise = torch.tensor(np.random.gamma(shape_param, scale_param, input.shape)).float().to(input.device)\n",
        "        input = input * noise\n",
        "\n",
        "        size = [self.num_func] + list(input.shape)\n",
        "        x = input.expand(size)\n",
        "        c = self.centers.view(-1, 1, 1, 1, 1)\n",
        "        weight = self.weight.view(-1, 1, self.num_filters, 1, 1)\n",
        "\n",
        "        if self.basis_func == gaussian:\n",
        "            distances = (x - c).div(self.gamma)\n",
        "            return self.basis_func(distances).mul(weight).sum(0), self.int_basis_func(distances, self.gamma).mul(weight).sum(0)\n",
        "        else:\n",
        "            distances = (x - c).abs()\n",
        "            return self.basis_func(distances, self.gamma).mul(weight).sum(0), 0\n",
        "\n",
        "# RBFs\n",
        "def gaussian(alpha):\n",
        "    phi = torch.exp(-0.5 * alpha.pow(2))\n",
        "    return phi\n",
        "\n",
        "def erf_func(alpha, gamma):\n",
        "    phi = gamma * math.sqrt(math.pi / 2) * torch.erf(alpha.div(math.sqrt(2)))\n",
        "    return phi\n",
        "\n",
        "def linear(alpha):\n",
        "    phi = alpha\n",
        "    return phi\n",
        "\n",
        "def quadratic(alpha):\n",
        "    phi = alpha.pow(2)\n",
        "    return phi\n",
        "\n",
        "def inverse_quadratic(alpha):\n",
        "    phi = torch.ones_like(alpha) / (torch.ones_like(alpha) + alpha.pow(2))\n",
        "    return phi\n",
        "\n",
        "def multiquadric(alpha):\n",
        "    phi = (torch.ones_like(alpha) + alpha.pow(2)).pow(0.5)\n",
        "    return phi\n",
        "\n",
        "def inverse_multiquadric(alpha):\n",
        "    phi = torch.ones_like(alpha) / (torch.ones_like(alpha) + alpha.pow(2)).pow(0.5)\n",
        "    return phi\n",
        "\n",
        "def spline(alpha):\n",
        "    phi = (alpha.pow(2) * torch.log(alpha + torch.ones_like(alpha)))\n",
        "    return phi\n",
        "\n",
        "def poisson_one(alpha):\n",
        "    phi = (alpha - torch.ones_like(alpha)) * torch.exp(-alpha)\n",
        "    return phi\n",
        "\n",
        "def poisson_two(alpha):\n",
        "    phi = ((alpha - 2 * torch.ones_like(alpha)) / 2 * torch.ones_like(alpha)) * alpha * torch.exp(-alpha)\n",
        "    return phi\n",
        "\n",
        "def matern32(alpha):\n",
        "    phi = (torch.ones_like(alpha) + 3**0.5 * alpha) * torch.exp(-3**0.5 * alpha)\n",
        "    return phi\n",
        "\n",
        "def matern52(alpha):\n",
        "    phi = (torch.ones_like(alpha) + 5**0.5 * alpha + (5 / 3) * alpha.pow(2)) * torch.exp(-5**0.5 * alpha)\n",
        "    return phi\n",
        "\n",
        "def triangular(alpha, gamma):\n",
        "    out = 1 - alpha.div(gamma)\n",
        "    out[alpha > gamma] = 0\n",
        "    return out\n",
        "\n",
        "def basis_func_dict():\n",
        "    \"\"\"\n",
        "    A helper function that returns a dictionary containing each RBF\n",
        "    \"\"\"\n",
        "    bases = {\n",
        "        'gaussian': gaussian,\n",
        "        'linear': linear,\n",
        "        'quadratic': quadratic,\n",
        "        'inverse quadratic': inverse_quadratic,\n",
        "        'multiquadric': multiquadric,\n",
        "        'inverse multiquadric': inverse_multiquadric,\n",
        "        'spline': spline,\n",
        "        'poisson one': poisson_one,\n",
        "        'poisson two': poisson_two,\n",
        "        'matern32': matern32,\n",
        "        'matern52': matern52\n",
        "    }\n",
        "    return bases\n",
        "\n"
      ],
      "metadata": {
        "id": "OX2Mbr-V-SRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "output:\n",
        "Epoch 2\n",
        "\n",
        "Epoch 3\n",
        "\n",
        "Epoch 4\n",
        "\n",
        "Epoch 5\n",
        "\n",
        "Epoch 6\n",
        "\n",
        "Epoch 7\n",
        "\n",
        "Epoch 8\n",
        "\n",
        "Epoch 9\n",
        "\n",
        "Epoch 10\n",
        "Iteration 101, G: 4880.41020, G_recon: 4880.410203, tnrd_loss: 0.000450,\n",
        "Evaluation: 15.340\n",
        "\n",
        "Epoch 11\n",
        "\n",
        "Epoch 12\n",
        "\n",
        "Epoch 13\n",
        "\n",
        "Epoch 14\n",
        "\n",
        "Epoch 15\n",
        "\n",
        "Epoch 16\n",
        "\n",
        "Epoch 17\n",
        "\n",
        "Epoch 18\n",
        "\n",
        "Epoch 19\n",
        "Iteration 201, G: 1990.89918, G_recon: 1990.899175, tnrd_loss: 0.000065,\n",
        "\n",
        "Epoch 20\n",
        "Evaluation: 16.154\n",
        "Best PSNR Score: 16.15"
      ],
      "metadata": {
        "id": "d5pI0Z7I-ehC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d_tHYSMP-ifW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataset.py\n",
        "\n"
      ],
      "metadata": {
        "id": "7UC1Jz5kjssz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "class DatasetNoise(torch.utils.data.dataset.Dataset):\n",
        "    def __init__(self, root='/content/denoising/data/FoETrainingSets180/', noise_sigma=50., training=True, crop_size=60, blind_denoising=False, gray_scale=False, max_size=None):\n",
        "        self.root = root\n",
        "        self.noise_sigma = noise_sigma\n",
        "        self.training = training\n",
        "        self.crop_size = crop_size\n",
        "        self.blind_denoising = blind_denoising\n",
        "        self.gray_scale = gray_scale\n",
        "        self.max_size = max_size\n",
        "\n",
        "        self._init()\n",
        "\n",
        "    def _init(self):\n",
        "        # data paths\n",
        "        targets = glob.glob(os.path.join(self.root, '*.*'))[:self.max_size]\n",
        "        self.paths = {'target' : targets}\n",
        "\n",
        "        # transforms\n",
        "        t_list = [transforms.ToTensor()]\n",
        "        self.image_transform = transforms.Compose(t_list)\n",
        "\n",
        "    def _get_augment_params(self, size):\n",
        "        random.seed(random.randint(0, 12345))\n",
        "\n",
        "        # position\n",
        "        w_size, h_size = size\n",
        "        x = random.randint(0, max(0, w_size - self.crop_size))\n",
        "        y = random.randint(0, max(0, h_size - self.crop_size))\n",
        "\n",
        "        # flip\n",
        "        flip = random.random() > 0.5\n",
        "        return {'crop_pos': (x, y), 'flip': flip}\n",
        "\n",
        "    def _augment(self, image, aug_params):\n",
        "        x, y = aug_params['crop_pos']\n",
        "        image = image.crop((x, y, x + self.crop_size, y + self.crop_size))\n",
        "        if aug_params['flip']:\n",
        "            image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "        return image\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # target image\n",
        "        if self.gray_scale:\n",
        "            target = Image.open(self.paths['target'][index]).convert('L')\n",
        "        else:\n",
        "            target = Image.open(self.paths['target'][index]).convert('RGB')\n",
        "\n",
        "        # transform\n",
        "        if self.training:\n",
        "            aug_params = self._get_augment_params(target.size)\n",
        "            target = self._augment(target, aug_params)\n",
        "        target = self.image_transform(target) * 255\n",
        "\n",
        "        # add multiplicative gamma noise\n",
        "        if self.blind_denoising:\n",
        "            noise_sigma = random.randint(0, self.noise_sigma)\n",
        "        else:\n",
        "            noise_sigma = self.noise_sigma\n",
        "\n",
        "        L = 1\n",
        "        a = L\n",
        "        b = 1 / L\n",
        "        #shape = target.shape\n",
        "        gamma_shape = target.shape # the shape of the gamma noise should match the image\n",
        "        gamma_noise = torch.from_numpy(np.random.gamma(shape=a, scale=b, size=gamma_shape)).float()\n",
        "\n",
        "        input = target * gamma_noise\n",
        "\n",
        "        return {'input': input, 'target': target, 'path': self.paths['target'][index]}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths['target'])\n",
        "\n",
        "# Usage\n",
        "# dataset = DatasetNoise(root='path_to_your_dataset', noise_sigma=50., training=True, crop_size=60, blind_denoising=False, gray_scale=False, max_size=None)\n",
        "# data_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "twovVhBOjrI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "add multiplicative gamma\n",
        "(dataset.py)->data"
      ],
      "metadata": {
        "id": "ey7DXSv830ev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "class DatasetNoise(torch.utils.data.dataset.Dataset):\n",
        "    def __init__(self, root='/content/denoising/data/FoETrainingSets180/', noise_sigma=50., training=True, crop_size=60, blind_denoising=False, gray_scale=False, max_size=None):\n",
        "        self.root = root\n",
        "        self.noise_sigma = noise_sigma\n",
        "        self.training = training\n",
        "        self.crop_size = crop_size\n",
        "        self.blind_denoising = blind_denoising\n",
        "        self.gray_scale = gray_scale\n",
        "        self.max_size = max_size\n",
        "\n",
        "        self._init()\n",
        "\n",
        "    def _init(self):\n",
        "        # data paths\n",
        "        targets = glob.glob(os.path.join(self.root, '*.*'))[:self.max_size]\n",
        "        self.paths = {'target' : targets}\n",
        "\n",
        "        # transforms\n",
        "        t_list = [transforms.ToTensor()]\n",
        "        self.image_transform = transforms.Compose(t_list)\n",
        "\n",
        "    def _get_augment_params(self, size):\n",
        "        random.seed(random.randint(0, 12345))\n",
        "\n",
        "        # position\n",
        "        w_size, h_size = size\n",
        "        x = random.randint(0, max(0, w_size - self.crop_size))\n",
        "        y = random.randint(0, max(0, h_size - self.crop_size))\n",
        "\n",
        "        # flip\n",
        "        flip = random.random() > 0.5\n",
        "        return {'crop_pos': (x, y), 'flip': flip}\n",
        "\n",
        "    def _augment(self, image, aug_params):\n",
        "        x, y = aug_params['crop_pos']\n",
        "        image = image.crop((x, y, x + self.crop_size, y + self.crop_size))\n",
        "        if aug_params['flip']:\n",
        "            image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "        return image\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # target image\n",
        "        if self.gray_scale:\n",
        "            target = Image.open(self.paths['target'][index]).convert('L')\n",
        "        else:\n",
        "            target = Image.open(self.paths['target'][index]).convert('RGB')\n",
        "\n",
        "        # transform\n",
        "        if self.training:\n",
        "            aug_params = self._get_augment_params(target.size)\n",
        "            target = self._augment(target, aug_params)\n",
        "        target = self.image_transform(target) * 255\n",
        "\n",
        "        # add multiplicative gamma noise\n",
        "        #if self.blind_denoising:\n",
        "            #noise_sigma = random.randint(0, self.noise_sigma)\n",
        "        #else:\n",
        "            #noise_sigma = self.noise_sigma\n",
        "        L = 2\n",
        "        a = L\n",
        "        b = 1 / L\n",
        "        #shape = target.shape\n",
        "        gamma_shape = target.shape # the shape of the gamma noise should match the image\n",
        "        gamma_noise = torch.from_numpy(np.random.gamma(shape=a, scale=b, size=gamma_shape)).float()\n",
        "        #shape = target.shape\n",
        "        #gamma_shape = shape  # the shape of the gamma noise should match the image\n",
        "        #gamma_noise = torch.from_numpy(np.random.gamma(shape=1.0, scale=noise_sigma / 255.0, size=gamma_shape)).float()\n",
        "\n",
        "        input = target * gamma_noise\n",
        "\n",
        "        return {'input': input, 'target': target, 'path': self.paths['target'][index]}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths['target'])\n",
        "\n",
        "# Usage\n",
        "# dataset = DatasetNoise(root='path_to_your_dataset', noise_sigma=50., training=True, crop_size=60, blind_denoising=False, gray_scale=False, max_size=None)\n",
        "# data_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "b87zjvr03z2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "main.py"
      ],
      "metadata": {
        "id": "4Je5OGVq5J_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.spectral_norm import spectral_norm as SpectralNorm\n",
        "from .modules.activations import *\n",
        "import torchvision\n",
        "from .modules.idct2_weight import gen_dct2\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "_NRBF=63\n",
        "_DCT=False\n",
        "_TIE = False\n",
        "_BETA = False\n",
        "_C1x1 = False\n",
        "__all__ = ['g_tnrd','d_tnrd','TNRDlayer']\n",
        "\n",
        "def initialize_weights(net, scale=1.):\n",
        "    if not isinstance(net, list):\n",
        "        net = [net]\n",
        "    for layer in net:\n",
        "        for m in layer.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, TNRDConv2d) or isinstance(m, TNRDlayer):\n",
        "                nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n",
        "                m.weight.data *= scale  # for residual block\n",
        "                if not _TIE and isinstance(m, TNRDlayer):\n",
        "                    nn.init.kaiming_normal_(m.weight2, a=0, mode='fan_in')\n",
        "                    m.weight.data *= scale  # for residual block\n",
        "                if m.bias is not None and not isinstance(m, TNRDlayer):\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n",
        "                m.weight.data *= scale\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "def init_model_param(model,num_reb_kernels=63,filter_size=5,stage=8,init_weight_dct=_DCT):\n",
        "    w0 = np.load('w0_orig.npy')\n",
        "    w0 = np.histogram(np.random.randn(1000)*0.02,num_reb_kernels-1)[1] if _NRBF != 63 else w0\n",
        "    #\n",
        "    means=np.linspace(-310,310,num_reb_kernels)\n",
        "    precision=0.01\n",
        "    NumW = num_reb_kernels\n",
        "    step = 0.2\n",
        "    delta = 10\n",
        "\n",
        "    D = np.arange(-delta+means[0],means[-1]+delta,step)\n",
        "    D_mu = D.reshape(1,-1).repeat(NumW,0) - means.reshape(-1,1).repeat(D.size,1)\n",
        "    offsetD = D[1]\n",
        "    nD = D.size\n",
        "    G = np.exp(-0.5*precision*D_mu**2)\n",
        "    filtN =  filter_size**2 - 1\n",
        "    m = filter_size**2 - 1\n",
        "\n",
        "    ww = np.array(w0).reshape(-1,1).repeat(filtN,1)\n",
        "    cof_beta = np.eye(m,m)\n",
        "    #x0 = zeros(length(cof_beta(:)) + 1 + filtN*mfs.NumW, stage);\n",
        "    theta = [10, 5]+ np.ones(stage-2).tolist()\n",
        "    pp = [math.log(1.0)]+ (math.log(0.1)*np.ones(stage-1)).tolist()\n",
        "    # beta = [log(1) log(0.1)*ones(1,stage-1)];\n",
        "    i=-1\n",
        "    #import pdb; pdb.set_trace()\n",
        "    for module in model.modules():\n",
        "        if isinstance(module,TNRDlayer):\n",
        "            i+=1\n",
        "            init_layer_params(module,cof_beta, pp[i], ww*theta[i],init_weight_dct)\n",
        "\n",
        "def init_layer_params(m,beta,p,wt,init_weight_dct):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if init_weight_dct:\n",
        "            m.weight.copy_(torch.Tensor(beta))\n",
        "        else:\n",
        "            n = m.kernel_size**2 * m.in_channels\n",
        "            m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            if not _TIE and isinstance(m, TNRDlayer):\n",
        "                weight_rot180 = torch.rot90(torch.rot90(m.weight.data.detach(), 1, [2, 3]),1,[2,3])\n",
        "                m.weight2.data.copy_(weight_rot180)\n",
        "                #m.weight2.data.normal_(0, math.sqrt(2. / n))\n",
        "            #initialize_weights(m,0.02)\n",
        "        m.alpha.copy_(torch.Tensor([p]))\n",
        "        if m.act.weight.shape[-1]==24:\n",
        "            m.act.weight.copy_(torch.Tensor(wt).unsqueeze(1))\n",
        "\n",
        "\n",
        "class TNRDConv2d(nn.Conv2d):\n",
        "    \"\"\"docstring for TNRDConv2d.\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=5,\n",
        "                 stride=1, padding=2, dilation=1, groups=1, bias=True):\n",
        "        super(TNRDConv2d, self).__init__(in_channels, out_channels, kernel_size,\n",
        "                                      stride, padding, dilation, groups, bias)\n",
        "\n",
        "        self.act = RBF(63,self.in_channels,triangular)\n",
        "        self.alpha=nn.Parameter(torch.Tensor([0.9]))\n",
        "        self.beta=nn.Parameter(torch.Tensor([1]))\n",
        "        initialize_weights_dct([self], 0.02)\n",
        "        self.pad_input=torchvision.transforms.Pad(5,padding_mode='edge')\n",
        "        #initialize_weights([self.act], 0.00002)\n",
        "        self.counter = 0\n",
        "    def forward(self,input):\n",
        "        self.counter+=1\n",
        "        u,f=input\n",
        "        up = self.pad_input(u)\n",
        "        ur = up.repeat(1,self.in_channels,1,1)\n",
        "        #\n",
        "        output1 = F.conv2d(ur, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
        "        output,_ = self.act(output1)\n",
        "        weight_rot180 = torch.rot90(torch.rot90(self.weight, 1, [2, 3]),1,[2,3])\n",
        "        #\n",
        "        output = F.conv2d(output, weight_rot180, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
        "        output = F.pad(output,(-5,-5,-5,-5))\n",
        "        #import pdb; pdb.set_trace()\n",
        "        if self.counter%100==0:\n",
        "            print(self.alpha,self.beta.max(),self.beta.min(),self.act.weight.max(),self.act.weight.min(),output.sum(1,keepdim=True).max())\n",
        "        output = u-self.beta*output.sum(1,keepdim=True)-self.alpha*(u-f)\n",
        "        return output,f\n",
        "\n",
        "class TNRDlayer(nn.Module):\n",
        "    \"\"\"docstring for TNRDConv2d.\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=5,\n",
        "                 stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
        "        super(TNRDlayer, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.stride=stride\n",
        "        self.groups=groups\n",
        "        self.bias=bias\n",
        "        self.padding=padding\n",
        "        self.stride=stride\n",
        "        self.dilation=dilation\n",
        "        self.kernel_size=kernel_size\n",
        "        self.act = RBF(_NRBF,self.in_channels,gaussian)\n",
        "        self.alpha=nn.Parameter(torch.Tensor([0.9]))\n",
        "        self.beta=nn.Parameter(torch.zeros([1,self.in_channels,1,1]))\n",
        "        if _DCT:\n",
        "            self.weight = nn.Parameter(torch.zeros([in_channels,in_channels]))\n",
        "        else:\n",
        "            self.weight = nn.Parameter(torch.zeros([in_channels,1,kernel_size,kernel_size]))\n",
        "            if not _TIE:\n",
        "                self.weight2 = nn.Parameter(torch.zeros([in_channels,1,kernel_size,kernel_size]))\n",
        "        if _C1x1:\n",
        "            self.weight_1x1=nn.Parameter(torch.zeros([in_channels,in_channels,1,1]))\n",
        "\n",
        "        #initialize_weights_dct([self], 0.02)\n",
        "        self.register_buffer('dct_filters',torch.tensor(gen_dct2(kernel_size)[1:,:]).float())\n",
        "\n",
        "        self.pad_input=torchvision.transforms.Pad(12,padding_mode='symmetric')\n",
        "        #initialize_weights([self.act], 0.00002)\n",
        "        self.counter = 0\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.counter += 1\n",
        "        u, f = input\n",
        "        u_prev = u\n",
        "\n",
        "        # Ensure to detach tensors before converting to numpy\n",
        "        u_cpu = u.detach().cpu().numpy()\n",
        "        u_t = np.zeros_like(u_cpu)\n",
        "\n",
        "        for it in range(1):\n",
        "            up = self.pad_input(u)\n",
        "            ur = up.repeat(1, self.in_channels, 1, 1)\n",
        "\n",
        "            if _DCT:\n",
        "                K = self.weight.matmul(self.dct_filters)\n",
        "                K = K.div(torch.norm(K, dim=1, keepdim=True) + 2.2e-16).view(self.kernel_size**2 - 1, 1, self.kernel_size, self.kernel_size)\n",
        "            else:\n",
        "                K = self.weight\n",
        "\n",
        "            output1 = F.conv2d(ur, K, None, self.stride, self.padding, self.dilation, self.groups)\n",
        "            output, _ = self.act(output1)\n",
        "            weight_rot180 = torch.rot90(torch.rot90(K, 1, [2, 3]), 1, [2, 3]) if _TIE else self.weight2\n",
        "\n",
        "            if _C1x1:\n",
        "                output = F.conv2d(output, self.weight_1x1, None, self.stride, self.padding, self.dilation)\n",
        "            output = F.conv2d(output, weight_rot180, None, self.stride, self.padding, self.dilation, self.groups)\n",
        "            output = F.pad(output, (-8, -8, -8, -8))\n",
        "\n",
        "            if self.counter % 500 == 0:\n",
        "                print(self.alpha, self.beta.max(), self.beta.min(), self.act.weight.max(), self.act.weight.min(), output.sum(1, keepdim=True).max())\n",
        "            beta = self.beta if _BETA else 1\n",
        "            u_next = ((self.alpha.exp()+2)/(1+self.alpha.exp())) * u - (1/(1+self.alpha.exp())) * u_prev + output.mul(beta).sum(1, keepdim=True) - self.alpha.exp() * ((u - f)/(u**2+1))\n",
        "\n",
        "            u_prev = u\n",
        "            u = u_next\n",
        "\n",
        "        return u, f\n",
        "\n",
        "class GenBlock(nn.Module):\n",
        "    def __init__(self, in_channels=64, out_channels=64, kernel_size=5, bias=True):\n",
        "        super(GenBlock, self).__init__()\n",
        "        self.conv = TNRDConv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=(kernel_size // 2), bias=bias)\n",
        "        self.bn = nn.BatchNorm2d(num_features=out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        initialize_weights([self.conv, self.bn], 0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn(self.conv(x)))\n",
        "        return x\n",
        "\n",
        "class DisBlock(nn.Module):\n",
        "    def __init__(self, in_channels=64, out_channels=64, bias=True, normalization=False):\n",
        "        super(DisBlock, self).__init__()\n",
        "        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1, bias=bias)\n",
        "        self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=4, stride=2, padding=1, bias=bias)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels, affine=True)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels, affine=True)\n",
        "\n",
        "        initialize_weights([self.conv1, self.conv2], 0.1)\n",
        "\n",
        "        if normalization:\n",
        "            self.conv1 = SpectralNorm(self.conv1)\n",
        "            self.conv2 = SpectralNorm(self.conv2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lrelu(self.bn1(self.conv1(x)))\n",
        "        x = self.lrelu(self.bn2(self.conv2(x)))\n",
        "        return x\n",
        "\n",
        "class Generatorv1(nn.Module):\n",
        "    def __init__(self, in_channels, num_features, gen_blocks, dis_blocks):\n",
        "        super(Generatorv1, self).__init__()\n",
        "        filter_size=5\n",
        "        # image to features\n",
        "        in_channels=filter_size**2\n",
        "\n",
        "        #self.crop_output=torchvision.transforms.CenterCrop(50)\n",
        "        self.image_to_features = TNRDConv2d(in_channels=in_channels, out_channels=in_channels,kernel_size=filter_size, groups=in_channels)\n",
        "        # features\n",
        "        blocks = []\n",
        "        for _ in range(gen_blocks):\n",
        "            blocks.append(TNRDConv2d(in_channels=in_channels, out_channels=in_channels,kernel_size=filter_size, bias=False,groups=in_channels))\n",
        "        self.features = nn.Sequential(*blocks)\n",
        "\n",
        "        # features to image\n",
        "        self.features_to_image = TNRDConv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=filter_size,groups=in_channels)\n",
        "        initialize_weights([self.features_to_image], 0.02)\n",
        "        self.counter=0\n",
        "    def forward(self, x):\n",
        "        self.counter+=1\n",
        "        x = self.image_to_features([x,x])\n",
        "        x = self.features(x)\n",
        "        x = self.features_to_image(x)\n",
        "        return x[0]\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_channels, num_features, gen_blocks, dis_blocks):\n",
        "        super(Generator, self).__init__()\n",
        "        filter_size=5\n",
        "        # image to features\n",
        "        in_channels=(filter_size**2-1)\n",
        "\n",
        "        #self.crop_output=torchvision.transforms.CenterCrop(50)\n",
        "        self.image_to_features = TNRDlayer(in_channels=in_channels, out_channels=in_channels,groups=in_channels)\n",
        "        # features\n",
        "        blocks = []\n",
        "        for _ in range(gen_blocks):\n",
        "            blocks.append(TNRDlayer(in_channels=in_channels, out_channels=in_channels, bias=False,groups=in_channels))\n",
        "        self.features = nn.Sequential(*blocks)\n",
        "\n",
        "        #self.features = []\n",
        "        #for _ in range(gen_blocks):\n",
        "        #    self.features.append(TNRDlayer(in_channels=in_channels, out_channels=in_channels, bias=False,groups=in_channels))\n",
        "\n",
        "        # features to image\n",
        "        self.features_to_image = TNRDlayer(in_channels=in_channels, out_channels=in_channels, kernel_size=5,groups=in_channels)\n",
        "        #initialize_weights([self.features_to_image], 0.02)\n",
        "        self.counter=0\n",
        "        init_model_param(self,num_reb_kernels=_NRBF,filter_size=5,stage=gen_blocks+2)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        self.counter+=1\n",
        "        x = self.image_to_features([x,x])\n",
        "        x = self.features(x)\n",
        "        x = self.features_to_image(x)\n",
        "        return x[0]\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels, num_features, gen_blocks, dis_blocks):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        # image to features\n",
        "        self.image_to_features = DisBlock(in_channels=in_channels, out_channels=num_features, bias=True, normalization=False)\n",
        "\n",
        "        # features\n",
        "        blocks = []\n",
        "        for i in range(0, dis_blocks - 1):\n",
        "            blocks.append(DisBlock(in_channels=num_features * min(pow(2, i), 8), out_channels=num_features * min(pow(2, i + 1), 8), bias=False, normalization=False))\n",
        "        self.features = nn.Sequential(*blocks)\n",
        "\n",
        "        # classifier\n",
        "        self.classifier = nn.Conv2d(in_channels=num_features * min(pow(2, dis_blocks - 1), 8), out_channels=1, kernel_size=4, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.image_to_features(x)\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        x = x.flatten(start_dim=1).mean(dim=-1)\n",
        "        return x\n",
        "\n",
        "class SNDiscriminator(nn.Module):\n",
        "    def __init__(self, in_channels, num_features, gen_blocks, dis_blocks):\n",
        "        super(SNDiscriminator, self).__init__()\n",
        "\n",
        "        # image to features\n",
        "        self.image_to_features = DisBlock(in_channels=in_channels, out_channels=num_features, bias=True, normalization=True)\n",
        "\n",
        "        # features\n",
        "        blocks = []\n",
        "        for i in range(0, dis_blocks - 1):\n",
        "            blocks.append(DisBlock(in_channels=num_features * min(pow(2, i), 8), out_channels=num_features * min(pow(2, i + 1), 8), bias=False, normalization=True))\n",
        "        self.features = nn.Sequential(*blocks)\n",
        "\n",
        "        # classifier\n",
        "        self.classifier = SpectralNorm(nn.Conv2d(in_channels=num_features * min(pow(2, dis_blocks - 1), 8), out_channels=1, kernel_size=4, padding=0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.image_to_features(x)\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        x = x.flatten(start_dim=1).mean(dim=-1)\n",
        "        return x\n",
        "\n",
        "def g_tnrd(**config):\n",
        "    config.setdefault('in_channels', 3)\n",
        "    config.setdefault('num_features', 64)\n",
        "    config.setdefault('gen_blocks', 3)\n",
        "    config.setdefault('dis_blocks', 5)\n",
        "\n",
        "    return Generator(**config)\n",
        "\n",
        "def d_tnrd(**config):\n",
        "    config.setdefault('in_channels', 3)\n",
        "    config.setdefault('num_features', 64)\n",
        "    config.setdefault('gen_blocks', 8)\n",
        "    config.setdefault('dis_blocks', 5)\n",
        "\n",
        "    return Discriminator(**config)"
      ],
      "metadata": {
        "id": "-0H_jgun5F6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "output:\n",
        "Epoch 2\n",
        "\n",
        "Epoch 3\n",
        "\n",
        "Epoch 4\n",
        "\n",
        "Epoch 5\n",
        "\n",
        "Epoch 6\n",
        "\n",
        "Epoch 7\n",
        "\n",
        "Epoch 8\n",
        "\n",
        "Epoch 9\n",
        "\n",
        "Epoch 10\n",
        "Iteration 101, G: 3948.60185, G_recon: 3948.601853, tnrd_loss: 0.442866,\n",
        "Evaluation: 12.871\n",
        "\n",
        "Epoch 11\n",
        "\n",
        "Epoch 12\n",
        "\n",
        "Epoch 13\n",
        "\n",
        "Epoch 14\n",
        "\n",
        "Epoch 15\n",
        "\n",
        "Epoch 16\n",
        "\n",
        "Epoch 17\n",
        "\n",
        "Epoch 18\n",
        "\n",
        "Epoch 19\n",
        "Iteration 201, G: 3182.31451, G_recon: 3182.314515, tnrd_loss: 26.381798,\n",
        "\n",
        "Epoch 20\n",
        "Evaluation: 14.530\n",
        "Best PSNR Score: 14.53\n",
        "\n",
        "\n",
        "\n",
        "\"here we only modified this two file of TNRD 'main.py' and 'dataset.py'  \""
      ],
      "metadata": {
        "id": "JeHRc6035Tol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**if we do not change 'main.py' only change 'dataset.py'(only add multiplicative gamma noise )**"
      ],
      "metadata": {
        "id": "MsFy_I4L960y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.spectral_norm import spectral_norm as SpectralNorm\n",
        "from .modules.activations import *\n",
        "import torchvision\n",
        "from .modules.idct2_weight import gen_dct2\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "_NRBF=63\n",
        "_DCT=False\n",
        "_TIE = False\n",
        "_BETA = False\n",
        "_C1x1 = False\n",
        "__all__ = ['g_tnrd','d_tnrd','TNRDlayer']\n",
        "\n",
        "def initialize_weights(net, scale=1.):\n",
        "    if not isinstance(net, list):\n",
        "        net = [net]\n",
        "    for layer in net:\n",
        "        for m in layer.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, TNRDConv2d) or isinstance(m, TNRDlayer):\n",
        "                nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n",
        "                m.weight.data *= scale  # for residual block\n",
        "                if not _TIE and isinstance(m, TNRDlayer):\n",
        "                    nn.init.kaiming_normal_(m.weight2, a=0, mode='fan_in')\n",
        "                    m.weight.data *= scale  # for residual block\n",
        "                if m.bias is not None and not isinstance(m, TNRDlayer):\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n",
        "                m.weight.data *= scale\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "def init_model_param(model,num_reb_kernels=63,filter_size=5,stage=8,init_weight_dct=_DCT):\n",
        "    w0 = np.load('w0_orig.npy')\n",
        "    w0 = np.histogram(np.random.randn(1000)*0.02,num_reb_kernels-1)[1] if _NRBF != 63 else w0\n",
        "    #\n",
        "    means=np.linspace(-310,310,num_reb_kernels)\n",
        "    precision=0.01\n",
        "    NumW = num_reb_kernels\n",
        "    step = 0.2\n",
        "    delta = 10\n",
        "\n",
        "    D = np.arange(-delta+means[0],means[-1]+delta,step)\n",
        "    D_mu = D.reshape(1,-1).repeat(NumW,0) - means.reshape(-1,1).repeat(D.size,1)\n",
        "    offsetD = D[1]\n",
        "    nD = D.size\n",
        "    G = np.exp(-0.5*precision*D_mu**2)\n",
        "    filtN =  filter_size**2 - 1\n",
        "    m = filter_size**2 - 1\n",
        "\n",
        "    ww = np.array(w0).reshape(-1,1).repeat(filtN,1)\n",
        "    cof_beta = np.eye(m,m)\n",
        "    #x0 = zeros(length(cof_beta(:)) + 1 + filtN*mfs.NumW, stage);\n",
        "    theta = [10, 5]+ np.ones(stage-2).tolist()\n",
        "    pp = [math.log(1.0)]+ (math.log(0.1)*np.ones(stage-1)).tolist()\n",
        "    # beta = [log(1) log(0.1)*ones(1,stage-1)];\n",
        "    i=-1\n",
        "    #import pdb; pdb.set_trace()\n",
        "    for module in model.modules():\n",
        "        if isinstance(module,TNRDlayer):\n",
        "            i+=1\n",
        "            init_layer_params(module,cof_beta, pp[i], ww*theta[i],init_weight_dct)\n",
        "\n",
        "def init_layer_params(m,beta,p,wt,init_weight_dct):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if init_weight_dct:\n",
        "            m.weight.copy_(torch.Tensor(beta))\n",
        "        else:\n",
        "            n = m.kernel_size**2 * m.in_channels\n",
        "            m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            if not _TIE and isinstance(m, TNRDlayer):\n",
        "                weight_rot180 = torch.rot90(torch.rot90(m.weight.data.detach(), 1, [2, 3]),1,[2,3])\n",
        "                m.weight2.data.copy_(weight_rot180)\n",
        "                #m.weight2.data.normal_(0, math.sqrt(2. / n))\n",
        "            #initialize_weights(m,0.02)\n",
        "        m.alpha.copy_(torch.Tensor([p]))\n",
        "        if m.act.weight.shape[-1]==24:\n",
        "            m.act.weight.copy_(torch.Tensor(wt).unsqueeze(1))\n",
        "\n",
        "\n",
        "class TNRDConv2d(nn.Conv2d):\n",
        "    \"\"\"docstring for TNRDConv2d.\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=5,\n",
        "                 stride=1, padding=2, dilation=1, groups=1, bias=True):\n",
        "        super(TNRDConv2d, self).__init__(in_channels, out_channels, kernel_size,\n",
        "                                      stride, padding, dilation, groups, bias)\n",
        "\n",
        "        self.act = RBF(63,self.in_channels,triangular)\n",
        "        self.alpha=nn.Parameter(torch.Tensor([0.9]))\n",
        "        self.beta=nn.Parameter(torch.Tensor([1]))\n",
        "        initialize_weights_dct([self], 0.02)\n",
        "        self.pad_input=torchvision.transforms.Pad(5,padding_mode='edge')\n",
        "        #initialize_weights([self.act], 0.00002)\n",
        "        self.counter = 0\n",
        "    def forward(self,input):\n",
        "        self.counter+=1\n",
        "        u,f=input\n",
        "        up = self.pad_input(u)\n",
        "        ur = up.repeat(1,self.in_channels,1,1)\n",
        "        #\n",
        "        output1 = F.conv2d(ur, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
        "        output,_ = self.act(output1)\n",
        "        weight_rot180 = torch.rot90(torch.rot90(self.weight, 1, [2, 3]),1,[2,3])\n",
        "        #\n",
        "        output = F.conv2d(output, weight_rot180, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
        "        output = F.pad(output,(-5,-5,-5,-5))\n",
        "        #import pdb; pdb.set_trace()\n",
        "        if self.counter%100==0:\n",
        "            print(self.alpha,self.beta.max(),self.beta.min(),self.act.weight.max(),self.act.weight.min(),output.sum(1,keepdim=True).max())\n",
        "        output = u-self.beta*output.sum(1,keepdim=True)-self.alpha*(u-f)\n",
        "        return output,f\n",
        "\n",
        "class TNRDlayer(nn.Module):\n",
        "    \"\"\"docstring for TNRDConv2d.\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=5,\n",
        "                 stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
        "        super(TNRDlayer, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.stride=stride\n",
        "        self.groups=groups\n",
        "        self.bias=bias\n",
        "        self.padding=padding\n",
        "        self.stride=stride\n",
        "        self.dilation=dilation\n",
        "        self.kernel_size=kernel_size\n",
        "        self.act = RBF(_NRBF,self.in_channels,gaussian)\n",
        "        self.alpha=nn.Parameter(torch.Tensor([0.9]))\n",
        "        self.beta=nn.Parameter(torch.zeros([1,self.in_channels,1,1]))\n",
        "        if _DCT:\n",
        "            self.weight = nn.Parameter(torch.zeros([in_channels,in_channels]))\n",
        "        else:\n",
        "            self.weight = nn.Parameter(torch.zeros([in_channels,1,kernel_size,kernel_size]))\n",
        "            if not _TIE:\n",
        "                self.weight2 = nn.Parameter(torch.zeros([in_channels,1,kernel_size,kernel_size]))\n",
        "        if _C1x1:\n",
        "            self.weight_1x1=nn.Parameter(torch.zeros([in_channels,in_channels,1,1]))\n",
        "\n",
        "        #initialize_weights_dct([self], 0.02)\n",
        "        self.register_buffer('dct_filters',torch.tensor(gen_dct2(kernel_size)[1:,:]).float())\n",
        "\n",
        "        self.pad_input=torchvision.transforms.Pad(12,padding_mode='symmetric')\n",
        "        #initialize_weights([self.act], 0.00002)\n",
        "        self.counter = 0\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.counter += 1\n",
        "        u, f = input\n",
        "        u_prev = u\n",
        "\n",
        "        # Ensure to detach tensors before converting to numpy\n",
        "        u_cpu = u.detach().cpu().numpy()\n",
        "        u_t = np.zeros_like(u_cpu)\n",
        "\n",
        "        for it in range(1):\n",
        "            up = self.pad_input(u)\n",
        "            ur = up.repeat(1, self.in_channels, 1, 1)\n",
        "\n",
        "            if _DCT:\n",
        "                K = self.weight.matmul(self.dct_filters)\n",
        "                K = K.div(torch.norm(K, dim=1, keepdim=True) + 2.2e-16).view(self.kernel_size**2 - 1, 1, self.kernel_size, self.kernel_size)\n",
        "            else:\n",
        "                K = self.weight\n",
        "\n",
        "            output1 = F.conv2d(ur, K, None, self.stride, self.padding, self.dilation, self.groups)\n",
        "            output, _ = self.act(output1)\n",
        "            weight_rot180 = torch.rot90(torch.rot90(K, 1, [2, 3]), 1, [2, 3]) if _TIE else self.weight2\n",
        "\n",
        "            if _C1x1:\n",
        "                output = F.conv2d(output, self.weight_1x1, None, self.stride, self.padding, self.dilation)\n",
        "            output = F.conv2d(output, weight_rot180, None, self.stride, self.padding, self.dilation, self.groups)\n",
        "            output = F.pad(output, (-8, -8, -8, -8))\n",
        "\n",
        "            if self.counter % 500 == 0:\n",
        "                print(self.alpha, self.beta.max(), self.beta.min(), self.act.weight.max(), self.act.weight.min(), output.sum(1, keepdim=True).max())\n",
        "            beta = self.beta if _BETA else 1\n",
        "            u_next = ((self.alpha.exp()+2)/(1+self.alpha.exp())) * u - (1/(1+self.alpha.exp())) * u_prev + output.mul(beta).sum(1, keepdim=True) - self.alpha.exp() * ((u - f)/(u**2+1))\n",
        "\n",
        "            u_prev = u\n",
        "            u = u_next\n",
        "\n",
        "        return u, f\n",
        "\n",
        "class GenBlock(nn.Module):\n",
        "    def __init__(self, in_channels=64, out_channels=64, kernel_size=5, bias=True):\n",
        "        super(GenBlock, self).__init__()\n",
        "        self.conv = TNRDConv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=(kernel_size // 2), bias=bias)\n",
        "        self.bn = nn.BatchNorm2d(num_features=out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        initialize_weights([self.conv, self.bn], 0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn(self.conv(x)))\n",
        "        return x\n",
        "\n",
        "class DisBlock(nn.Module):\n",
        "    def __init__(self, in_channels=64, out_channels=64, bias=True, normalization=False):\n",
        "        super(DisBlock, self).__init__()\n",
        "        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1, bias=bias)\n",
        "        self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=4, stride=2, padding=1, bias=bias)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels, affine=True)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels, affine=True)\n",
        "\n",
        "        initialize_weights([self.conv1, self.conv2], 0.1)\n",
        "\n",
        "        if normalization:\n",
        "            self.conv1 = SpectralNorm(self.conv1)\n",
        "            self.conv2 = SpectralNorm(self.conv2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lrelu(self.bn1(self.conv1(x)))\n",
        "        x = self.lrelu(self.bn2(self.conv2(x)))\n",
        "        return x\n",
        "\n",
        "class Generatorv1(nn.Module):\n",
        "    def __init__(self, in_channels, num_features, gen_blocks, dis_blocks):\n",
        "        super(Generatorv1, self).__init__()\n",
        "        filter_size=5\n",
        "        # image to features\n",
        "        in_channels=filter_size**2\n",
        "\n",
        "        #self.crop_output=torchvision.transforms.CenterCrop(50)\n",
        "        self.image_to_features = TNRDConv2d(in_channels=in_channels, out_channels=in_channels,kernel_size=filter_size, groups=in_channels)\n",
        "        # features\n",
        "        blocks = []\n",
        "        for _ in range(gen_blocks):\n",
        "            blocks.append(TNRDConv2d(in_channels=in_channels, out_channels=in_channels,kernel_size=filter_size, bias=False,groups=in_channels))\n",
        "        self.features = nn.Sequential(*blocks)\n",
        "\n",
        "        # features to image\n",
        "        self.features_to_image = TNRDConv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=filter_size,groups=in_channels)\n",
        "        initialize_weights([self.features_to_image], 0.02)\n",
        "        self.counter=0\n",
        "    def forward(self, x):\n",
        "        self.counter+=1\n",
        "        x = self.image_to_features([x,x])\n",
        "        x = self.features(x)\n",
        "        x = self.features_to_image(x)\n",
        "        return x[0]\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_channels, num_features, gen_blocks, dis_blocks):\n",
        "        super(Generator, self).__init__()\n",
        "        filter_size=5\n",
        "        # image to features\n",
        "        in_channels=(filter_size**2-1)\n",
        "\n",
        "        #self.crop_output=torchvision.transforms.CenterCrop(50)\n",
        "        self.image_to_features = TNRDlayer(in_channels=in_channels, out_channels=in_channels,groups=in_channels)\n",
        "        # features\n",
        "        blocks = []\n",
        "        for _ in range(gen_blocks):\n",
        "            blocks.append(TNRDlayer(in_channels=in_channels, out_channels=in_channels, bias=False,groups=in_channels))\n",
        "        self.features = nn.Sequential(*blocks)\n",
        "\n",
        "        #self.features = []\n",
        "        #for _ in range(gen_blocks):\n",
        "        #    self.features.append(TNRDlayer(in_channels=in_channels, out_channels=in_channels, bias=False,groups=in_channels))\n",
        "\n",
        "        # features to image\n",
        "        self.features_to_image = TNRDlayer(in_channels=in_channels, out_channels=in_channels, kernel_size=5,groups=in_channels)\n",
        "        #initialize_weights([self.features_to_image], 0.02)\n",
        "        self.counter=0\n",
        "        init_model_param(self,num_reb_kernels=_NRBF,filter_size=5,stage=gen_blocks+2)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        self.counter+=1\n",
        "        x = self.image_to_features([x,x])\n",
        "        x = self.features(x)\n",
        "        x = self.features_to_image(x)\n",
        "        return x[0]\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels, num_features, gen_blocks, dis_blocks):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        # image to features\n",
        "        self.image_to_features = DisBlock(in_channels=in_channels, out_channels=num_features, bias=True, normalization=False)\n",
        "\n",
        "        # features\n",
        "        blocks = []\n",
        "        for i in range(0, dis_blocks - 1):\n",
        "            blocks.append(DisBlock(in_channels=num_features * min(pow(2, i), 8), out_channels=num_features * min(pow(2, i + 1), 8), bias=False, normalization=False))\n",
        "        self.features = nn.Sequential(*blocks)\n",
        "\n",
        "        # classifier\n",
        "        self.classifier = nn.Conv2d(in_channels=num_features * min(pow(2, dis_blocks - 1), 8), out_channels=1, kernel_size=4, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.image_to_features(x)\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        x = x.flatten(start_dim=1).mean(dim=-1)\n",
        "        return x\n",
        "\n",
        "class SNDiscriminator(nn.Module):\n",
        "    def __init__(self, in_channels, num_features, gen_blocks, dis_blocks):\n",
        "        super(SNDiscriminator, self).__init__()\n",
        "\n",
        "        # image to features\n",
        "        self.image_to_features = DisBlock(in_channels=in_channels, out_channels=num_features, bias=True, normalization=True)\n",
        "\n",
        "        # features\n",
        "        blocks = []\n",
        "        for i in range(0, dis_blocks - 1):\n",
        "            blocks.append(DisBlock(in_channels=num_features * min(pow(2, i), 8), out_channels=num_features * min(pow(2, i + 1), 8), bias=False, normalization=True))\n",
        "        self.features = nn.Sequential(*blocks)\n",
        "\n",
        "        # classifier\n",
        "        self.classifier = SpectralNorm(nn.Conv2d(in_channels=num_features * min(pow(2, dis_blocks - 1), 8), out_channels=1, kernel_size=4, padding=0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.image_to_features(x)\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        x = x.flatten(start_dim=1).mean(dim=-1)\n",
        "        return x\n",
        "\n",
        "def g_tnrd(**config):\n",
        "    config.setdefault('in_channels', 3)\n",
        "    config.setdefault('num_features', 64)\n",
        "    config.setdefault('gen_blocks', 3)\n",
        "    config.setdefault('dis_blocks', 5)\n",
        "\n",
        "    return Generator(**config)\n",
        "\n",
        "def d_tnrd(**config):\n",
        "    config.setdefault('in_channels', 3)\n",
        "    config.setdefault('num_features', 64)\n",
        "    config.setdefault('gen_blocks', 8)\n",
        "    config.setdefault('dis_blocks', 5)\n",
        "\n",
        "    return Discriminator(**config)"
      ],
      "metadata": {
        "id": "qzFWQxKN9tLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Akt7PD9k9wH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch 2\n",
        "\n",
        "Epoch 3\n",
        "\n",
        "Epoch 4\n",
        "\n",
        "Epoch 5\n",
        "\n",
        "Epoch 6\n",
        "\n",
        "Epoch 7\n",
        "\n",
        "Epoch 8\n",
        "\n",
        "Epoch 9\n",
        "\n",
        "Epoch 10\n",
        "Iteration 101, G: 3925.91670, G_recon: 3925.916704, tnrd_loss: 0.580104,\n",
        "Evaluation: 12.828\n",
        "\n",
        "Epoch 11\n",
        "\n",
        "Epoch 12\n",
        "\n",
        "Epoch 13\n",
        "\n",
        "Epoch 14\n",
        "\n",
        "Epoch 15\n",
        "\n",
        "Epoch 16\n",
        "\n",
        "Epoch 17\n",
        "\n",
        "Epoch 18\n",
        "\n",
        "Epoch 19\n",
        "Iteration 201, G: 3282.31828, G_recon: 3282.318278, tnrd_loss: 19.670256,\n",
        "\n",
        "Epoch 20\n",
        "Evaluation: 14.354\n",
        "Best PSNR Score: 14.35\n",
        "█$ colab>>  "
      ],
      "metadata": {
        "id": "4mtLVd3j924Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "only change ->activation.py"
      ],
      "metadata": {
        "id": "1NzKTZEGl_-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import numpy as np\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# RBF Layer\n",
        "\n",
        "# RBF Layer\n",
        "class RBF(nn.Module):\n",
        "    \"\"\"\n",
        "    Transforms incoming data using a given radial basis function:\n",
        "    u_{i} = rbf(||x - c_{i}|| / s_{i})\n",
        "    Arguments:\n",
        "        in_features: size of each input sample\n",
        "        out_features: size of each output sample\n",
        "    Shape:\n",
        "        - Input: (N, in_features) where N is an arbitrary batch size\n",
        "        - Output: (N, out_features) where N is an arbitrary batch size\n",
        "    Attributes:\n",
        "        centers: the learnable centers of shape (out_features, in_features).\n",
        "            The values are initialised from a standard normal distribution.\n",
        "            Normalising inputs to have mean 0 and standard deviation 1 is\n",
        "            recommended.\n",
        "\n",
        "        sigmas: the learnable scaling factors of shape (out_features).\n",
        "            The values are initialised as ones.\n",
        "\n",
        "        basis_func: the radial basis function used to transform the scaled\n",
        "            distances.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_func, num_filters, basis_func):\n",
        "        super(RBF, self).__init__()\n",
        "        self.num_func = num_func\n",
        "        self.register_buffer('centers', torch.tensor(np.linspace(-310, 310, num_func)).float())\n",
        "        self.num_filters = num_filters\n",
        "        self.weight = nn.Parameter(torch.Tensor(num_func, 1, num_filters))\n",
        "        self.gamma = 10\n",
        "        self.basis_func = basis_func\n",
        "        self.int_basis_func = erf_func\n",
        "\n",
        "    def forward(self, input, shape_param=2.0, scale_param=1.0):\n",
        "        # Generate and apply multiplicative gamma noise\n",
        "        noise = torch.tensor(np.random.gamma(shape_param, scale_param, input.shape)).float().to(input.device)\n",
        "        input = input * noise\n",
        "\n",
        "        size = [self.num_func] + list(input.shape)\n",
        "        x = input.expand(size)\n",
        "        c = self.centers.view(-1, 1, 1, 1, 1)\n",
        "        weight = self.weight.view(-1, 1, self.num_filters, 1, 1)\n",
        "\n",
        "        if self.basis_func == gaussian:\n",
        "            distances = (x - c).div(self.gamma)\n",
        "            return self.basis_func(distances).mul(weight).sum(0), self.int_basis_func(distances, self.gamma).mul(weight).sum(0)\n",
        "        else:\n",
        "            distances = (x - c).abs()\n",
        "            return self.basis_func(distances, self.gamma).mul(weight).sum(0), 0\n",
        "\n",
        "# RBFs\n",
        "def gaussian(alpha):\n",
        "    phi = torch.exp(-0.5 * alpha.pow(2))\n",
        "    return phi\n",
        "\n",
        "def erf_func(alpha, gamma):\n",
        "    phi = gamma * math.sqrt(math.pi / 2) * torch.erf(alpha.div(math.sqrt(2)))\n",
        "    return phi\n",
        "\n",
        "def linear(alpha):\n",
        "    phi = alpha\n",
        "    return phi\n",
        "\n",
        "def quadratic(alpha):\n",
        "    phi = alpha.pow(2)\n",
        "    return phi\n",
        "\n",
        "def inverse_quadratic(alpha):\n",
        "    phi = torch.ones_like(alpha) / (torch.ones_like(alpha) + alpha.pow(2))\n",
        "    return phi\n",
        "\n",
        "def multiquadric(alpha):\n",
        "    phi = (torch.ones_like(alpha) + alpha.pow(2)).pow(0.5)\n",
        "    return phi\n",
        "\n",
        "def inverse_multiquadric(alpha):\n",
        "    phi = torch.ones_like(alpha) / (torch.ones_like(alpha) + alpha.pow(2)).pow(0.5)\n",
        "    return phi\n",
        "\n",
        "def spline(alpha):\n",
        "    phi = (alpha.pow(2) * torch.log(alpha + torch.ones_like(alpha)))\n",
        "    return phi\n",
        "\n",
        "def poisson_one(alpha):\n",
        "    phi = (alpha - torch.ones_like(alpha)) * torch.exp(-alpha)\n",
        "    return phi\n",
        "\n",
        "def poisson_two(alpha):\n",
        "    phi = ((alpha - 2 * torch.ones_like(alpha)) / 2 * torch.ones_like(alpha)) * alpha * torch.exp(-alpha)\n",
        "    return phi\n",
        "\n",
        "def matern32(alpha):\n",
        "    phi = (torch.ones_like(alpha) + 3**0.5 * alpha) * torch.exp(-3**0.5 * alpha)\n",
        "    return phi\n",
        "\n",
        "def matern52(alpha):\n",
        "    phi = (torch.ones_like(alpha) + 5**0.5 * alpha + (5 / 3) * alpha.pow(2)) * torch.exp(-5**0.5 * alpha)\n",
        "    return phi\n",
        "\n",
        "def triangular(alpha, gamma):\n",
        "    out = 1 - alpha.div(gamma)\n",
        "    out[alpha > gamma] = 0\n",
        "    return out\n",
        "\n",
        "def basis_func_dict():\n",
        "    \"\"\"\n",
        "    A helper function that returns a dictionary containing each RBF\n",
        "    \"\"\"\n",
        "    bases = {\n",
        "        'gaussian': gaussian,\n",
        "        'linear': linear,\n",
        "        'quadratic': quadratic,\n",
        "        'inverse quadratic': inverse_quadratic,\n",
        "        'multiquadric': multiquadric,\n",
        "        'inverse multiquadric': inverse_multiquadric,\n",
        "        'spline': spline,\n",
        "        'poisson one': poisson_one,\n",
        "        'poisson two': poisson_two,\n",
        "        'matern32': matern32,\n",
        "        'matern52': matern52\n",
        "    }\n",
        "    return bases\n",
        "\n"
      ],
      "metadata": {
        "id": "LwMAsgBsl_WL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "output:\n",
        "\n",
        "Epoch 2\n",
        "\n",
        "Epoch 3\n",
        "\n",
        "Epoch 4\n",
        "\n",
        "Epoch 5\n",
        "\n",
        "Epoch 6\n",
        "\n",
        "Epoch 7\n",
        "\n",
        "Epoch 8\n",
        "\n",
        "Epoch 9\n",
        "\n",
        "Epoch 10\n",
        "Iteration 101, G: 1625.46031, G_recon: 1625.460313, tnrd_loss: 0.000000,\n",
        "Evaluation: 18.860\n",
        "\n",
        "Epoch 11\n",
        "\n",
        "Epoch 12\n",
        "\n",
        "Epoch 13\n",
        "\n",
        "Epoch 14\n",
        "\n",
        "Epoch 15\n",
        "\n",
        "Epoch 16\n",
        "\n",
        "Epoch 17\n",
        "\n",
        "Epoch 18\n",
        "\n",
        "Epoch 19\n",
        "Iteration 201, G: 496.42950, G_recon: 496.429504, tnrd_loss: 0.000000,\n",
        "\n",
        "Epoch 20\n",
        "Evaluation: 24.862\n",
        "Best PSNR Score: 24.86\n",
        "█$ colab>>  python main.py --root /content/denoising/data/FoETrainingSets180/ --g-model g_tnrd\n",
        "--d-model d_tnrd --model-config \"{'gen_blocks':3, 'dis_blocks':4, 'in_channels':1}\" --reconstru\n",
        "ction-weight 1.0 --perceptual-weight 0 --adversarial-weight 0 --tnrd-energy-weight 0 --crop-siz\n",
        "e 100 --gray-scale --noise-sigma 50 --epochs 20 --step-size 150 --batch-size 2 --eval-every 10\n",
        "--print-every 100 --lr 1e-3\n",
        "Namespace(device='cuda', device_ids=[0], d_model='d_tnrd', g_model='g_tnrd', model_config=\"{'ge\n",
        "n_blocks':3, 'dis_blocks':4, 'in_channels':1}\", dis_to_load='', gen_to_load='', root='/content/\n",
        "denoising/data/FoETrainingSets180/', noise_sigma=50, crop_size=100, gray_scale=True, blind=Fals\n",
        "e, max_size=None, num_workers=2, batch_size=2, epochs=20, lr=0.001, gen_betas=[0.9, 0.99], dis_\n",
        "betas=[0.9, 0.99], num_critic=1, wgan=False, relativistic=False, step_size=150, gamma=0.5, pena\n",
        "lty_weight=0, range_weight=0, reconstruction_weight=1.0, tnrd_energy_weight=0.0, greedy_weight=\n",
        "0.0, perceptual_weight=0.0, adversarial_weight=0.0, textural_weight=0, print_every=100, eval_ev\n",
        "ery=10, results_dir='./results', save='2024-07-13_10-13-37', evaluation=False, use_tb=False, sa\n",
        "ve_path='./results/2024-07-13_10-13-37')\n",
        "Generator(\n",
        "  (image_to_features): TNRDlayer(\n",
        "    (act): RBF()\n",
        "    (pad_input): Pad(padding=12, fill=0, padding_mode=symmetric)\n",
        "  )\n",
        "  (features): Sequential(\n",
        "    (0): TNRDlayer(\n",
        "      (act): RBF()\n",
        "      (pad_input): Pad(padding=12, fill=0, padding_mode=symmetric)\n",
        "    )\n",
        "    (1): TNRDlayer(\n",
        "      (act): RBF()\n",
        "      (pad_input): Pad(padding=12, fill=0, padding_mode=symmetric)\n",
        "    )\n",
        "    (2): TNRDlayer(\n",
        "      (act): RBF()\n",
        "      (pad_input): Pad(padding=12, fill=0, padding_mode=symmetric)\n",
        "    )\n",
        "  )\n",
        "  (features_to_image): TNRDlayer(\n",
        "    (act): RBF()\n",
        "    (pad_input): Pad(padding=12, fill=0, padding_mode=symmetric)\n",
        "  )\n",
        ")\n",
        "Number of parameters in generator: 13685\n",
        "\n",
        "Discriminator(\n",
        "  (image_to_features): DisBlock(\n",
        "    (lrelu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "    (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (conv2): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
        "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  )\n",
        "  (features): Sequential(\n",
        "    (0): DisBlock(\n",
        "      (lrelu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "      (conv2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    )\n",
        "    (1): DisBlock(\n",
        "      (lrelu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "      (conv2): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    )\n",
        "    (2): DisBlock(\n",
        "      (lrelu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "      (conv2): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    )\n",
        "  )\n",
        "  (classifier): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1))\n",
        ")\n",
        "Number of parameters in discriminator: 7131585\n",
        "\n",
        "\n",
        "Epoch 1\n",
        "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch\n",
        "parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Plea\n",
        "se use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different\n",
        "from None, the closed form is used instead of the new chainable form, where available. Please o\n",
        "pen an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/i\n",
        "ssues/new/choose.\n",
        "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
        "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected\n",
        "call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should\n",
        "call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to\n",
        "do this will result in PyTorch skipping the first value of the learning rate schedule. See more\n",
        "details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
        "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
        "\n",
        "Epoch 2\n",
        "\n",
        "Epoch 3\n",
        "\n",
        "Epoch 4\n",
        "\n",
        "Epoch 5\n",
        "\n",
        "Epoch 6\n",
        "\n",
        "Epoch 7\n",
        "\n",
        "Epoch 8\n",
        "\n",
        "Epoch 9\n",
        "\n",
        "Epoch 10\n",
        "Iteration 101, G: 1652.38937, G_recon: 1652.389373, tnrd_loss: 0.000000,\n",
        "Evaluation: 18.571\n",
        "\n",
        "Epoch 11\n",
        "\n",
        "Epoch 12\n",
        "\n",
        "Epoch 13\n",
        "\n",
        "Epoch 14\n",
        "\n",
        "Epoch 15\n",
        "\n",
        "Epoch 16\n",
        "\n",
        "Epoch 17\n",
        "\n",
        "Epoch 18\n",
        "\n",
        "Epoch 19\n",
        "Iteration 201, G: 532.80635, G_recon: 532.806354, tnrd_loss: 0.000000,\n",
        "\n",
        "Epoch 20\n",
        "Evaluation: 24.601\n",
        "Best PSNR Score: 24.60\n",
        "█$ colab>> █$ colab>>  python main.py --root /content/denoising/data/FoETrainingSets180/ --g-model g_tnrd--d-model d_tnrd --model-config \"{'gen_blocks':3, 'dis_blocks':4, 'in_channels':1}\" --reconstruction-weight 1.0 --perceptual-weight 0 --adversarial-weight 0 --tnrd-energy-weight 0 --crop-size 100 --gray-scale --noise-sigma 50 --epochs 20 --step-size 150 --batch-size 2 --eval-every 10--print-every 100 --lr 1e-3\n"
      ],
      "metadata": {
        "id": "QpT0MCuomUAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k=5\n",
        "for i in range(7):\n",
        "  k=k+1\n",
        "  print(i)\n",
        "  print(k)"
      ],
      "metadata": {
        "id": "AooCa_whmYMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.spectral_norm import spectral_norm as SpectralNorm\n",
        "from .modules.activations import *\n",
        "import torchvision\n",
        "from .modules.idct2_weight import gen_dct2\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "_NRBF=63\n",
        "_DCT=False\n",
        "_TIE = False\n",
        "_BETA = False\n",
        "_C1x1 = False\n",
        "__all__ = ['g_tnrd','d_tnrd','TNRDlayer']\n",
        "\n",
        "def initialize_weights(net, scale=1.):\n",
        "    if not isinstance(net, list):\n",
        "        net = [net]\n",
        "    for layer in net:\n",
        "        for m in layer.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, TNRDConv2d) or isinstance(m, TNRDlayer):\n",
        "                nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n",
        "                m.weight.data *= scale  # for residual block\n",
        "                if not _TIE and isinstance(m, TNRDlayer):\n",
        "                    nn.init.kaiming_normal_(m.weight2, a=0, mode='fan_in')\n",
        "                    m.weight.data *= scale  # for residual block\n",
        "                if m.bias is not None and not isinstance(m, TNRDlayer):\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n",
        "                m.weight.data *= scale\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "def init_model_param(model,num_reb_kernels=63,filter_size=5,stage=8,init_weight_dct=_DCT):\n",
        "    w0 = np.load('w0_orig.npy')\n",
        "    w0 = np.histogram(np.random.randn(1000)*0.02,num_reb_kernels-1)[1] if _NRBF != 63 else w0\n",
        "    #\n",
        "    means=np.linspace(-310,310,num_reb_kernels)\n",
        "    precision=0.01\n",
        "    NumW = num_reb_kernels\n",
        "    step = 0.2\n",
        "    delta = 10\n",
        "\n",
        "    D = np.arange(-delta+means[0],means[-1]+delta,step)\n",
        "    D_mu = D.reshape(1,-1).repeat(NumW,0) - means.reshape(-1,1).repeat(D.size,1)\n",
        "    offsetD = D[1]\n",
        "    nD = D.size\n",
        "    G = np.exp(-0.5*precision*D_mu**2)\n",
        "    filtN =  filter_size**2 - 1\n",
        "    m = filter_size**2 - 1\n",
        "\n",
        "    ww = np.array(w0).reshape(-1,1).repeat(filtN,1)\n",
        "    cof_beta = np.eye(m,m)\n",
        "    #x0 = zeros(length(cof_beta(:)) + 1 + filtN*mfs.NumW, stage);\n",
        "    theta = [10, 5]+ np.ones(stage-2).tolist()\n",
        "    pp = [math.log(1.0)]+ (math.log(0.1)*np.ones(stage-1)).tolist()\n",
        "    # beta = [log(1) log(0.1)*ones(1,stage-1)];\n",
        "    i=-1\n",
        "    #import pdb; pdb.set_trace()\n",
        "    for module in model.modules():\n",
        "        if isinstance(module,TNRDlayer):\n",
        "            i+=1\n",
        "            init_layer_params(module,cof_beta, pp[i], ww*theta[i],init_weight_dct)\n",
        "\n",
        "def init_layer_params(m,beta,p,wt,init_weight_dct):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if init_weight_dct:\n",
        "            m.weight.copy_(torch.Tensor(beta))\n",
        "        else:\n",
        "            n = m.kernel_size**2 * m.in_channels\n",
        "            m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            if not _TIE and isinstance(m, TNRDlayer):\n",
        "                weight_rot180 = torch.rot90(torch.rot90(m.weight.data.detach(), 1, [2, 3]),1,[2,3])\n",
        "                m.weight2.data.copy_(weight_rot180)\n",
        "                #m.weight2.data.normal_(0, math.sqrt(2. / n))\n",
        "            #initialize_weights(m,0.02)\n",
        "        m.alpha.copy_(torch.Tensor([p]))\n",
        "        if m.act.weight.shape[-1]==24:\n",
        "            m.act.weight.copy_(torch.Tensor(wt).unsqueeze(1))\n",
        "\n",
        "\n",
        "class TNRDConv2d(nn.Conv2d):\n",
        "    \"\"\"docstring for TNRDConv2d.\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=5,\n",
        "                 stride=1, padding=2, dilation=1, groups=1, bias=True):\n",
        "        super(TNRDConv2d, self).__init__(in_channels, out_channels, kernel_size,\n",
        "                                      stride, padding, dilation, groups, bias)\n",
        "\n",
        "        self.act = RBF(63,self.in_channels,triangular)\n",
        "        self.alpha=nn.Parameter(torch.Tensor([0.9]))\n",
        "        self.beta=nn.Parameter(torch.Tensor([1]))\n",
        "        initialize_weights_dct([self], 0.02)\n",
        "        self.pad_input=torchvision.transforms.Pad(5,padding_mode='edge')\n",
        "        #initialize_weights([self.act], 0.00002)\n",
        "        self.counter = 0\n",
        "    def forward(self,input):\n",
        "        self.counter+=1\n",
        "        u,f=input\n",
        "        u_prev=u\n",
        "        up = self.pad_input(u)\n",
        "        ur = up.repeat(1,self.in_channels,1,1)\n",
        "        #\n",
        "        output1 = F.conv2d(ur, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
        "        output,_ = self.act(output1)\n",
        "        weight_rot180 = torch.rot90(torch.rot90(self.weight, 1, [2, 3]),1,[2,3])\n",
        "        #\n",
        "        output = F.conv2d(output, weight_rot180, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
        "        output = F.pad(output,(-5,-5,-5,-5))\n",
        "        #import pdb; pdb.set_trace()\n",
        "        if self.counter%100==0:\n",
        "            print(self.alpha,self.beta.max(),self.beta.min(),self.act.weight.max(),self.act.weight.min(),output.sum(1,keepdim=True).max())\n",
        "        #output = u-self.beta*output.sum(1,keepdim=True)-self.alpha*(u-f)\n",
        "        output = ((self.alpha.exp()+2)/(1+self.alpha.exp())) * u-(1/(1+self.alpha.exp())) * u_prev-self.beta*output.sum(1,keepdim=True)-self.alpha.exp() * ((u - f)/(u**2+1))\n",
        "        u_prev=u\n",
        "        u=output             ################### MODIFIED_HERE\n",
        "        # u_next = ((self.alpha.exp()+2)/(1+self.alpha.exp())) * u - (1/(1+self.alpha.exp())) * u_prev + output.mul(beta).sum(1, keepdim=True) - self.alpha.exp() * ((u - f)/(u**2+1))\n",
        "        return output,f\n",
        "\n",
        "class TNRDlayer(nn.Module):\n",
        "    \"\"\"docstring for TNRDConv2d.\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=5,\n",
        "                 stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
        "        super(TNRDlayer, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.stride=stride\n",
        "        self.groups=groups\n",
        "        self.bias=bias\n",
        "        self.padding=padding\n",
        "        self.stride=stride\n",
        "        self.dilation=dilation\n",
        "        self.kernel_size=kernel_size\n",
        "        self.act = RBF(_NRBF,self.in_channels,gaussian)\n",
        "        self.alpha=nn.Parameter(torch.Tensor([0.9]))\n",
        "        self.beta=nn.Parameter(torch.zeros([1,self.in_channels,1,1]))\n",
        "        if _DCT:\n",
        "            self.weight = nn.Parameter(torch.zeros([in_channels,in_channels]))\n",
        "        else:\n",
        "            self.weight = nn.Parameter(torch.zeros([in_channels,1,kernel_size,kernel_size]))\n",
        "            if not _TIE:\n",
        "                self.weight2 = nn.Parameter(torch.zeros([in_channels,1,kernel_size,kernel_size]))\n",
        "        if _C1x1:\n",
        "            self.weight_1x1=nn.Parameter(torch.zeros([in_channels,in_channels,1,1]))\n",
        "\n",
        "        #initialize_weights_dct([self], 0.02)\n",
        "        self.register_buffer('dct_filters',torch.tensor(gen_dct2(kernel_size)[1:,:]).float())\n",
        "\n",
        "        self.pad_input=torchvision.transforms.Pad(12,padding_mode='symmetric')\n",
        "        #initialize_weights([self.act], 0.00002)\n",
        "        self.counter = 0\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.counter += 1\n",
        "        u, f = input\n",
        "        u_prev = u\n",
        "\n",
        "        # Ensure to detach tensors before converting to numpy\n",
        "        u_cpu = u.detach().cpu().numpy()\n",
        "        u_t = np.zeros_like(u_cpu)\n",
        "\n",
        "        for it in range(1):\n",
        "            up = self.pad_input(u)\n",
        "            ur = up.repeat(1, self.in_channels, 1, 1)\n",
        "\n",
        "            if _DCT:\n",
        "                K = self.weight.matmul(self.dct_filters)\n",
        "                K = K.div(torch.norm(K, dim=1, keepdim=True) + 2.2e-16).view(self.kernel_size**2 - 1, 1, self.kernel_size, self.kernel_size)\n",
        "            else:\n",
        "                K = self.weight\n",
        "\n",
        "            output1 = F.conv2d(ur, K, None, self.stride, self.padding, self.dilation, self.groups)\n",
        "            output, _ = self.act(output1)\n",
        "            weight_rot180 = torch.rot90(torch.rot90(K, 1, [2, 3]), 1, [2, 3]) if _TIE else self.weight2\n",
        "\n",
        "            if _C1x1:\n",
        "                output = F.conv2d(output, self.weight_1x1, None, self.stride, self.padding, self.dilation)\n",
        "            output = F.conv2d(output, weight_rot180, None, self.stride, self.padding, self.dilation, self.groups)\n",
        "            output = F.pad(output, (-8, -8, -8, -8))\n",
        "\n",
        "            if self.counter % 500 == 0:\n",
        "                print(self.alpha, self.beta.max(), self.beta.min(), self.act.weight.max(), self.act.weight.min(), output.sum(1, keepdim=True).max())\n",
        "            beta = self.beta if _BETA else 1\n",
        "            u_next = ((self.alpha.exp()+2)/(1+self.alpha.exp())) * u - (1/(1+self.alpha.exp())) * u_prev + output.mul(beta).sum(1, keepdim=True) - self.alpha.exp() * ((u - f)/(u**2+1))\n",
        "\n",
        "            u_prev = u\n",
        "            u = u_next\n",
        "\n",
        "        return u, f\n",
        "\n",
        "class GenBlock(nn.Module):\n",
        "    def __init__(self, in_channels=64, out_channels=64, kernel_size=5, bias=True):\n",
        "        super(GenBlock, self).__init__()\n",
        "        self.conv = TNRDConv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=(kernel_size // 2), bias=bias)\n",
        "        self.bn = nn.BatchNorm2d(num_features=out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        initialize_weights([self.conv, self.bn], 0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn(self.conv(x)))\n",
        "        return x\n",
        "\n",
        "class DisBlock(nn.Module):\n",
        "    def __init__(self, in_channels=64, out_channels=64, bias=True, normalization=False):\n",
        "        super(DisBlock, self).__init__()\n",
        "        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1, bias=bias)\n",
        "        self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=4, stride=2, padding=1, bias=bias)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels, affine=True)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels, affine=True)\n",
        "\n",
        "        initialize_weights([self.conv1, self.conv2], 0.1)\n",
        "\n",
        "        if normalization:\n",
        "            self.conv1 = SpectralNorm(self.conv1)\n",
        "            self.conv2 = SpectralNorm(self.conv2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lrelu(self.bn1(self.conv1(x)))\n",
        "        x = self.lrelu(self.bn2(self.conv2(x)))\n",
        "        return x\n",
        "\n",
        "class Generatorv1(nn.Module):\n",
        "    def __init__(self, in_channels, num_features, gen_blocks, dis_blocks):\n",
        "        super(Generatorv1, self).__init__()\n",
        "        filter_size=5\n",
        "        # image to features\n",
        "        in_channels=filter_size**2\n",
        "\n",
        "        #self.crop_output=torchvision.transforms.CenterCrop(50)\n",
        "        self.image_to_features = TNRDConv2d(in_channels=in_channels, out_channels=in_channels,kernel_size=filter_size, groups=in_channels)\n",
        "        # features\n",
        "        blocks = []\n",
        "        for _ in range(gen_blocks):\n",
        "            blocks.append(TNRDConv2d(in_channels=in_channels, out_channels=in_channels,kernel_size=filter_size, bias=False,groups=in_channels))\n",
        "        self.features = nn.Sequential(*blocks)\n",
        "\n",
        "        # features to image\n",
        "        self.features_to_image = TNRDConv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=filter_size,groups=in_channels)\n",
        "        initialize_weights([self.features_to_image], 0.02)\n",
        "        self.counter=0\n",
        "    def forward(self, x):\n",
        "        self.counter+=1\n",
        "        x = self.image_to_features([x,x])\n",
        "        x = self.features(x)\n",
        "        x = self.features_to_image(x)\n",
        "        return x[0]\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_channels, num_features, gen_blocks, dis_blocks):\n",
        "        super(Generator, self).__init__()\n",
        "        filter_size=5\n",
        "        # image to features\n",
        "        in_channels=(filter_size**2-1)\n",
        "\n",
        "        #self.crop_output=torchvision.transforms.CenterCrop(50)\n",
        "        self.image_to_features = TNRDlayer(in_channels=in_channels, out_channels=in_channels,groups=in_channels)\n",
        "        # features\n",
        "        blocks = []\n",
        "        for _ in range(gen_blocks):\n",
        "            blocks.append(TNRDlayer(in_channels=in_channels, out_channels=in_channels, bias=False,groups=in_channels))\n",
        "        self.features = nn.Sequential(*blocks)\n",
        "\n",
        "        #self.features = []\n",
        "        #for _ in range(gen_blocks):\n",
        "        #    self.features.append(TNRDlayer(in_channels=in_channels, out_channels=in_channels, bias=False,groups=in_channels))\n",
        "\n",
        "        # features to image\n",
        "        self.features_to_image = TNRDlayer(in_channels=in_channels, out_channels=in_channels, kernel_size=5,groups=in_channels)\n",
        "        #initialize_weights([self.features_to_image], 0.02)\n",
        "        self.counter=0\n",
        "        init_model_param(self,num_reb_kernels=_NRBF,filter_size=5,stage=gen_blocks+2)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        self.counter+=1\n",
        "        x = self.image_to_features([x,x])\n",
        "        x = self.features(x)\n",
        "        x = self.features_to_image(x)\n",
        "        return x[0]\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels, num_features, gen_blocks, dis_blocks):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        # image to features\n",
        "        self.image_to_features = DisBlock(in_channels=in_channels, out_channels=num_features, bias=True, normalization=False)\n",
        "\n",
        "        # features\n",
        "        blocks = []\n",
        "        for i in range(0, dis_blocks - 1):\n",
        "            blocks.append(DisBlock(in_channels=num_features * min(pow(2, i), 8), out_channels=num_features * min(pow(2, i + 1), 8), bias=False, normalization=False))\n",
        "        self.features = nn.Sequential(*blocks)\n",
        "\n",
        "        # classifier\n",
        "        self.classifier = nn.Conv2d(in_channels=num_features * min(pow(2, dis_blocks - 1), 8), out_channels=1, kernel_size=4, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.image_to_features(x)\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        x = x.flatten(start_dim=1).mean(dim=-1)\n",
        "        return x\n",
        "\n",
        "class SNDiscriminator(nn.Module):\n",
        "    def __init__(self, in_channels, num_features, gen_blocks, dis_blocks):\n",
        "        super(SNDiscriminator, self).__init__()\n",
        "\n",
        "        # image to features\n",
        "        self.image_to_features = DisBlock(in_channels=in_channels, out_channels=num_features, bias=True, normalization=True)\n",
        "\n",
        "        # features\n",
        "        blocks = []\n",
        "        for i in range(0, dis_blocks - 1):\n",
        "            blocks.append(DisBlock(in_channels=num_features * min(pow(2, i), 8), out_channels=num_features * min(pow(2, i + 1), 8), bias=False, normalization=True))\n",
        "        self.features = nn.Sequential(*blocks)\n",
        "\n",
        "        # classifier\n",
        "        self.classifier = SpectralNorm(nn.Conv2d(in_channels=num_features * min(pow(2, dis_blocks - 1), 8), out_channels=1, kernel_size=4, padding=0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.image_to_features(x)\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        x = x.flatten(start_dim=1).mean(dim=-1)\n",
        "        return x\n",
        "\n",
        "def g_tnrd(**config):\n",
        "    config.setdefault('in_channels', 3)\n",
        "    config.setdefault('num_features', 64)\n",
        "    config.setdefault('gen_blocks', 3)\n",
        "    config.setdefault('dis_blocks', 5)\n",
        "\n",
        "    return Generator(**config)\n",
        "\n",
        "def d_tnrd(**config):\n",
        "    config.setdefault('in_channels', 3)\n",
        "    config.setdefault('num_features', 64)\n",
        "    config.setdefault('gen_blocks', 8)\n",
        "    config.setdefault('dis_blocks', 5)\n",
        "\n",
        "    return Discriminator(**config)"
      ],
      "metadata": {
        "id": "5F6t2FeHgRPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.spectral_norm import spectral_norm as SpectralNorm\n",
        "from .modules.activations import *\n",
        "import torchvision\n",
        "from .modules.idct2_weight import gen_dct2\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "_NRBF=63\n",
        "_DCT=False\n",
        "_TIE = False\n",
        "_BETA = False\n",
        "_C1x1 = False\n",
        "__all__ = ['g_tnrd','d_tnrd','TNRDlayer']\n",
        "\n",
        "def initialize_weights(net, scale=1.):\n",
        "    if not isinstance(net, list):\n",
        "        net = [net]\n",
        "    for layer in net:\n",
        "        for m in layer.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, TNRDConv2d) or isinstance(m, TNRDlayer):\n",
        "                nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n",
        "                m.weight.data *= scale  # for residual block\n",
        "                if not _TIE and isinstance(m, TNRDlayer):\n",
        "                    nn.init.kaiming_normal_(m.weight2, a=0, mode='fan_in')\n",
        "                    m.weight.data *= scale  # for residual block\n",
        "                if m.bias is not None and not isinstance(m, TNRDlayer):\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n",
        "                m.weight.data *= scale\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "def init_model_param(model,num_reb_kernels=63,filter_size=5,stage=8,init_weight_dct=_DCT):\n",
        "    w0 = np.load('w0_orig.npy')\n",
        "    w0 = np.histogram(np.random.randn(1000)*0.02,num_reb_kernels-1)[1] if _NRBF != 63 else w0\n",
        "    #\n",
        "    means=np.linspace(-310,310,num_reb_kernels)\n",
        "    precision=0.01\n",
        "    NumW = num_reb_kernels\n",
        "    step = 0.2\n",
        "    delta = 10\n",
        "\n",
        "    D = np.arange(-delta+means[0],means[-1]+delta,step)\n",
        "    D_mu = D.reshape(1,-1).repeat(NumW,0) - means.reshape(-1,1).repeat(D.size,1)\n",
        "    offsetD = D[1]\n",
        "    nD = D.size\n",
        "    G = np.exp(-0.5*precision*D_mu**2)\n",
        "    filtN =  filter_size**2 - 1\n",
        "    m = filter_size**2 - 1\n",
        "\n",
        "    ww = np.array(w0).reshape(-1,1).repeat(filtN,1)\n",
        "    cof_beta = np.eye(m,m)\n",
        "    #x0 = zeros(length(cof_beta(:)) + 1 + filtN*mfs.NumW, stage);\n",
        "    theta = [10, 5]+ np.ones(stage-2).tolist()\n",
        "    pp = [math.log(1.0)]+ (math.log(0.1)*np.ones(stage-1)).tolist()\n",
        "    # beta = [log(1) log(0.1)*ones(1,stage-1)];\n",
        "    i=-1\n",
        "    #import pdb; pdb.set_trace()\n",
        "    for module in model.modules():\n",
        "        if isinstance(module,TNRDlayer):\n",
        "            i+=1\n",
        "            init_layer_params(module,cof_beta, pp[i], ww*theta[i],init_weight_dct)\n",
        "\n",
        "def init_layer_params(m,beta,p,wt,init_weight_dct):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if init_weight_dct:\n",
        "            m.weight.copy_(torch.Tensor(beta))\n",
        "        else:\n",
        "            n = m.kernel_size**2 * m.in_channels\n",
        "            m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            if not _TIE and isinstance(m, TNRDlayer):\n",
        "                weight_rot180 = torch.rot90(torch.rot90(m.weight.data.detach(), 1, [2, 3]),1,[2,3])\n",
        "                m.weight2.data.copy_(weight_rot180)\n",
        "                #m.weight2.data.normal_(0, math.sqrt(2. / n))\n",
        "            #initialize_weights(m,0.02)\n",
        "        m.alpha.copy_(torch.Tensor([p]))\n",
        "        if m.act.weight.shape[-1]==24:\n",
        "            m.act.weight.copy_(torch.Tensor(wt).unsqueeze(1))\n",
        "\n",
        "\n",
        "class TNRDConv2d(nn.Conv2d):\n",
        "    \"\"\"docstring for TNRDConv2d.\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=5,\n",
        "                 stride=1, padding=2, dilation=1, groups=1, bias=True):\n",
        "        super(TNRDConv2d, self).__init__(in_channels, out_channels, kernel_size,\n",
        "                                      stride, padding, dilation, groups, bias)\n",
        "\n",
        "        self.act = RBF(63,self.in_channels,triangular)\n",
        "        self.alpha=nn.Parameter(torch.Tensor([0.9]))\n",
        "        self.beta=nn.Parameter(torch.Tensor([1]))\n",
        "        initialize_weights_dct([self], 0.02)\n",
        "        self.pad_input=torchvision.transforms.Pad(5,padding_mode='edge')\n",
        "        #initialize_weights([self.act], 0.00002)\n",
        "        self.counter = 0\n",
        "    def forward(self,input):\n",
        "        self.counter+=1\n",
        "        u,f=input\n",
        "        u_prev=u\n",
        "        up = self.pad_input(u)\n",
        "        ur = up.repeat(1,self.in_channels,1,1)\n",
        "        #\n",
        "        output1 = F.conv2d(ur, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
        "        output,_ = self.act(output1)\n",
        "        weight_rot180 = torch.rot90(torch.rot90(self.weight, 1, [2, 3]),1,[2,3])\n",
        "        #\n",
        "        output = F.conv2d(output, weight_rot180, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
        "        output = F.pad(output,(-5,-5,-5,-5))\n",
        "        #import pdb; pdb.set_trace()\n",
        "        if self.counter%100==0:\n",
        "            print(self.alpha,self.beta.max(),self.beta.min(),self.act.weight.max(),self.act.weight.min(),output.sum(1,keepdim=True).max())\n",
        "        #output = u-self.beta*output.sum(1,keepdim=True)-self.alpha*(u-f)\n",
        "        output = ((self.alpha.exp()+2)/(1+self.alpha.exp())) * u-(1/(1+self.alpha.exp())) * u_prev-self.beta*output.sum(1,keepdim=True)-self.alpha.exp() * ((u - f)/(u**2+1))\n",
        "        u_prev=u\n",
        "        u=output             ################### MODIFIED_HERE\n",
        "        # u_next = ((self.alpha.exp()+2)/(1+self.alpha.exp())) * u - (1/(1+self.alpha.exp())) * u_prev + output.mul(beta).sum(1, keepdim=True) - self.alpha.exp() * ((u - f)/(u**2+1))\n",
        "        return output,f\n",
        "\n",
        "class TNRDlayer(nn.Module):\n",
        "    \"\"\"docstring for TNRDConv2d.\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=5,\n",
        "                 stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
        "        super(TNRDlayer, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.stride=stride\n",
        "        self.groups=groups\n",
        "        self.bias=bias\n",
        "        self.padding=padding\n",
        "        self.stride=stride\n",
        "        self.dilation=dilation\n",
        "        self.kernel_size=kernel_size\n",
        "        self.act = RBF(_NRBF,self.in_channels,gaussian)\n",
        "        self.alpha=nn.Parameter(torch.Tensor([0.9]))\n",
        "        self.beta=nn.Parameter(torch.zeros([1,self.in_channels,1,1]))\n",
        "        if _DCT:\n",
        "            self.weight = nn.Parameter(torch.zeros([in_channels,in_channels]))\n",
        "        else:\n",
        "            self.weight = nn.Parameter(torch.zeros([in_channels,1,kernel_size,kernel_size]))\n",
        "            if not _TIE:\n",
        "                self.weight2 = nn.Parameter(torch.zeros([in_channels,1,kernel_size,kernel_size]))\n",
        "        if _C1x1:\n",
        "            self.weight_1x1=nn.Parameter(torch.zeros([in_channels,in_channels,1,1]))\n",
        "\n",
        "        #initialize_weights_dct([self], 0.02)\n",
        "        self.register_buffer('dct_filters',torch.tensor(gen_dct2(kernel_size)[1:,:]).float())\n",
        "\n",
        "        self.pad_input=torchvision.transforms.Pad(12,padding_mode='symmetric')\n",
        "        #initialize_weights([self.act], 0.00002)\n",
        "        self.counter = 0\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.counter += 1\n",
        "        u, f = input\n",
        "        u_prev = u\n",
        "\n",
        "        # Ensure to detach tensors before converting to numpy\n",
        "        u_cpu = u.detach().cpu().numpy()\n",
        "        u_t = np.zeros_like(u_cpu)\n",
        "\n",
        "        for it in range(1):\n",
        "            up = self.pad_input(u)\n",
        "            ur = up.repeat(1, self.in_channels, 1, 1)\n",
        "\n",
        "            if _DCT:\n",
        "                K = self.weight.matmul(self.dct_filters)\n",
        "                K = K.div(torch.norm(K, dim=1, keepdim=True) + 2.2e-16).view(self.kernel_size**2 - 1, 1, self.kernel_size, self.kernel_size)\n",
        "            else:\n",
        "                K = self.weight\n",
        "\n",
        "            output1 = F.conv2d(ur, K, None, self.stride, self.padding, self.dilation, self.groups)\n",
        "            output, _ = self.act(output1)\n",
        "            weight_rot180 = torch.rot90(torch.rot90(K, 1, [2, 3]), 1, [2, 3]) if _TIE else self.weight2\n",
        "\n",
        "            if _C1x1:\n",
        "                output = F.conv2d(output, self.weight_1x1, None, self.stride, self.padding, self.dilation)\n",
        "            output = F.conv2d(output, weight_rot180, None, self.stride, self.padding, self.dilation, self.groups)\n",
        "            output = F.pad(output, (-8, -8, -8, -8))\n",
        "\n",
        "            if self.counter % 500 == 0:\n",
        "                print(self.alpha, self.beta.max(), self.beta.min(), self.act.weight.max(), self.act.weight.min(), output.sum(1, keepdim=True).max())\n",
        "            beta = self.beta if _BETA else 1\n",
        "            u_next = ((self.alpha.exp()+2)/(1+self.alpha.exp())) * u - (1/(1+self.alpha.exp())) * u_prev + output.mul(beta).sum(1, keepdim=True) - self.alpha.exp() * ((u - f)/(u**2+1))\n",
        "\n",
        "            u_prev = u\n",
        "            u = u_next\n",
        "\n",
        "        return u, f\n",
        "\n",
        "class GenBlock(nn.Module):\n",
        "    def __init__(self, in_channels=64, out_channels=64, kernel_size=5, bias=True):\n",
        "        super(GenBlock, self).__init__()\n",
        "        self.conv = TNRDConv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=(kernel_size // 2), bias=bias)\n",
        "        self.bn = nn.BatchNorm2d(num_features=out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        initialize_weights([self.conv, self.bn], 0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn(self.conv(x)))\n",
        "        return x\n",
        "\n",
        "class DisBlock(nn.Module):\n",
        "    def __init__(self, in_channels=64, out_channels=64, bias=True, normalization=False):\n",
        "        super(DisBlock, self).__init__()\n",
        "        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1, bias=bias)\n",
        "        self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=4, stride=2, padding=1, bias=bias)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels, affine=True)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels, affine=True)\n",
        "\n",
        "        initialize_weights([self.conv1, self.conv2], 0.1)\n",
        "\n",
        "        if normalization:\n",
        "            self.conv1 = SpectralNorm(self.conv1)\n",
        "            self.conv2 = SpectralNorm(self.conv2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lrelu(self.bn1(self.conv1(x)))\n",
        "        x = self.lrelu(self.bn2(self.conv2(x)))\n",
        "        return x\n",
        "\n",
        "class Generatorv1(nn.Module):\n",
        "    def __init__(self, in_channels, num_features, gen_blocks, dis_blocks):\n",
        "        super(Generatorv1, self).__init__()\n",
        "        filter_size=5\n",
        "        # image to features\n",
        "        in_channels=filter_size**2\n",
        "\n",
        "        #self.crop_output=torchvision.transforms.CenterCrop(50)\n",
        "        self.image_to_features = TNRDConv2d(in_channels=in_channels, out_channels=in_channels,kernel_size=filter_size, groups=in_channels)\n",
        "        # features\n",
        "        blocks = []\n",
        "        for _ in range(gen_blocks):\n",
        "            blocks.append(TNRDConv2d(in_channels=in_channels, out_channels=in_channels,kernel_size=filter_size, bias=False,groups=in_channels))\n",
        "        self.features = nn.Sequential(*blocks)\n",
        "\n",
        "        # features to image\n",
        "        self.features_to_image = TNRDConv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=filter_size,groups=in_channels)\n",
        "        initialize_weights([self.features_to_image], 0.02)\n",
        "        self.counter=0\n",
        "    def forward(self, x):\n",
        "        self.counter+=1\n",
        "        x = self.image_to_features([x,x])\n",
        "        x = self.features(x)\n",
        "        x = self.features_to_image(x)\n",
        "        return x[0]\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_channels, num_features, gen_blocks, dis_blocks):\n",
        "        super(Generator, self).__init__()\n",
        "        filter_size=5\n",
        "        # image to features\n",
        "        in_channels=(filter_size**2-1)\n",
        "\n",
        "        #self.crop_output=torchvision.transforms.CenterCrop(50)\n",
        "        self.image_to_features = TNRDlayer(in_channels=in_channels, out_channels=in_channels,groups=in_channels)\n",
        "        # features\n",
        "        blocks = []\n",
        "        for _ in range(gen_blocks):\n",
        "            blocks.append(TNRDlayer(in_channels=in_channels, out_channels=in_channels, bias=False,groups=in_channels))\n",
        "        self.features = nn.Sequential(*blocks)\n",
        "\n",
        "        #self.features = []\n",
        "        #for _ in range(gen_blocks):\n",
        "        #    self.features.append(TNRDlayer(in_channels=in_channels, out_channels=in_channels, bias=False,groups=in_channels))\n",
        "\n",
        "        # features to image\n",
        "        self.features_to_image = TNRDlayer(in_channels=in_channels, out_channels=in_channels, kernel_size=5,groups=in_channels)\n",
        "        #initialize_weights([self.features_to_image], 0.02)\n",
        "        self.counter=0\n",
        "        init_model_param(self,num_reb_kernels=_NRBF,filter_size=5,stage=gen_blocks+2)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        self.counter+=1\n",
        "        x = self.image_to_features([x,x])\n",
        "        x = self.features(x)\n",
        "        x = self.features_to_image(x)\n",
        "        return x[0]\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels, num_features, gen_blocks, dis_blocks):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        # image to features\n",
        "        self.image_to_features = DisBlock(in_channels=in_channels, out_channels=num_features, bias=True, normalization=False)\n",
        "\n",
        "        # features\n",
        "        blocks = []\n",
        "        for i in range(0, dis_blocks - 1):\n",
        "            blocks.append(DisBlock(in_channels=num_features * min(pow(2, i), 8), out_channels=num_features * min(pow(2, i + 1), 8), bias=False, normalization=False))\n",
        "        self.features = nn.Sequential(*blocks)\n",
        "\n",
        "        # classifier\n",
        "        self.classifier = nn.Conv2d(in_channels=num_features * min(pow(2, dis_blocks - 1), 8), out_channels=1, kernel_size=4, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.image_to_features(x)\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        x = x.flatten(start_dim=1).mean(dim=-1)\n",
        "        return x\n",
        "\n",
        "class SNDiscriminator(nn.Module):\n",
        "    def __init__(self, in_channels, num_features, gen_blocks, dis_blocks):\n",
        "        super(SNDiscriminator, self).__init__()\n",
        "\n",
        "        # image to features\n",
        "        self.image_to_features = DisBlock(in_channels=in_channels, out_channels=num_features, bias=True, normalization=True)\n",
        "\n",
        "        # features\n",
        "        blocks = []\n",
        "        for i in range(0, dis_blocks - 1):\n",
        "            blocks.append(DisBlock(in_channels=num_features * min(pow(2, i), 8), out_channels=num_features * min(pow(2, i + 1), 8), bias=False, normalization=True))\n",
        "        self.features = nn.Sequential(*blocks)\n",
        "\n",
        "        # classifier\n",
        "        self.classifier = SpectralNorm(nn.Conv2d(in_channels=num_features * min(pow(2, dis_blocks - 1), 8), out_channels=1, kernel_size=4, padding=0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.image_to_features(x)\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        x = x.flatten(start_dim=1).mean(dim=-1)\n",
        "        return x\n",
        "\n",
        "def g_tnrd(**config):\n",
        "    config.setdefault('in_channels', 3)\n",
        "    config.setdefault('num_features', 64)\n",
        "    config.setdefault('gen_blocks', 3)\n",
        "    config.setdefault('dis_blocks', 5)\n",
        "\n",
        "    return Generator(**config)\n",
        "\n",
        "def d_tnrd(**config):\n",
        "    config.setdefault('in_channels', 3)\n",
        "    config.setdefault('num_features', 64)\n",
        "    config.setdefault('gen_blocks', 8)\n",
        "    config.setdefault('dis_blocks', 5)\n",
        "\n",
        "    return Discriminator(**config)"
      ],
      "metadata": {
        "id": "xU7-8V87pBny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Screenshot 2024-07-14 151403.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "hfT41lRnoIl8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**add above equation on --->>   tnrd.py(modules)**"
      ],
      "metadata": {
        "id": "mEjzlSe-obUi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eznV2MWKoILY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.spectral_norm import spectral_norm as SpectralNorm\n",
        "from .modules.activations import *\n",
        "import torchvision\n",
        "from .modules.idct2_weight import gen_dct2\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "_NRBF=63\n",
        "_DCT=False\n",
        "_TIE = False\n",
        "_BETA = False\n",
        "_C1x1 = False\n",
        "__all__ = ['g_tnrd','d_tnrd','TNRDlayer']\n",
        "\n",
        "def initialize_weights(net, scale=1.):\n",
        "    if not isinstance(net, list):\n",
        "        net = [net]\n",
        "    for layer in net:\n",
        "        for m in layer.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, TNRDConv2d) or isinstance(m, TNRDlayer):\n",
        "                nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n",
        "                m.weight.data *= scale  # for residual block\n",
        "                if not _TIE and isinstance(m, TNRDlayer):\n",
        "                    nn.init.kaiming_normal_(m.weight2, a=0, mode='fan_in')\n",
        "                    m.weight.data *= scale  # for residual block\n",
        "                if m.bias is not None and not isinstance(m, TNRDlayer):\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n",
        "                m.weight.data *= scale\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "def init_model_param(model,num_reb_kernels=63,filter_size=5,stage=8,init_weight_dct=_DCT):\n",
        "    w0 = np.load('w0_orig.npy')\n",
        "    w0 = np.histogram(np.random.randn(1000)*0.02,num_reb_kernels-1)[1] if _NRBF != 63 else w0\n",
        "    #\n",
        "    means=np.linspace(-310,310,num_reb_kernels)\n",
        "    precision=0.01\n",
        "    NumW = num_reb_kernels\n",
        "    step = 0.2\n",
        "    delta = 10\n",
        "\n",
        "    D = np.arange(-delta+means[0],means[-1]+delta,step)\n",
        "    D_mu = D.reshape(1,-1).repeat(NumW,0) - means.reshape(-1,1).repeat(D.size,1)\n",
        "    offsetD = D[1]\n",
        "    nD = D.size\n",
        "    G = np.exp(-0.5*precision*D_mu**2)\n",
        "    filtN =  filter_size**2 - 1\n",
        "    m = filter_size**2 - 1\n",
        "\n",
        "    ww = np.array(w0).reshape(-1,1).repeat(filtN,1)\n",
        "    cof_beta = np.eye(m,m)\n",
        "    #x0 = zeros(length(cof_beta(:)) + 1 + filtN*mfs.NumW, stage);\n",
        "    theta = [10, 5]+ np.ones(stage-2).tolist()\n",
        "    pp = [math.log(1.0)]+ (math.log(0.1)*np.ones(stage-1)).tolist()\n",
        "    # beta = [log(1) log(0.1)*ones(1,stage-1)];\n",
        "    i=-1\n",
        "    #import pdb; pdb.set_trace()\n",
        "    for module in model.modules():\n",
        "        if isinstance(module,TNRDlayer):\n",
        "            i+=1\n",
        "            init_layer_params(module,cof_beta, pp[i], ww*theta[i],init_weight_dct)\n",
        "\n",
        "def init_layer_params(m,beta,p,wt,init_weight_dct):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if init_weight_dct:\n",
        "            m.weight.copy_(torch.Tensor(beta))\n",
        "        else:\n",
        "            n = m.kernel_size**2 * m.in_channels\n",
        "            m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            if not _TIE and isinstance(m, TNRDlayer):\n",
        "                weight_rot180 = torch.rot90(torch.rot90(m.weight.data.detach(), 1, [2, 3]),1,[2,3])\n",
        "                m.weight2.data.copy_(weight_rot180)\n",
        "                #m.weight2.data.normal_(0, math.sqrt(2. / n))\n",
        "            #initialize_weights(m,0.02)\n",
        "        m.alpha.copy_(torch.Tensor([p]))\n",
        "        if m.act.weight.shape[-1]==24:\n",
        "            m.act.weight.copy_(torch.Tensor(wt).unsqueeze(1))\n",
        "\n",
        "\n",
        "class TNRDConv2d(nn.Conv2d):\n",
        "    \"\"\"docstring for TNRDConv2d.\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=5,\n",
        "                 stride=1, padding=2, dilation=1, groups=1, bias=True):\n",
        "        super(TNRDConv2d, self).__init__(in_channels, out_channels, kernel_size,\n",
        "                                      stride, padding, dilation, groups, bias)\n",
        "\n",
        "        self.act = RBF(63,self.in_channels,triangular)\n",
        "        self.alpha=nn.Parameter(torch.Tensor([0.9]))\n",
        "        self.beta=nn.Parameter(torch.Tensor([1]))\n",
        "        initialize_weights_dct([self], 0.02)\n",
        "        self.pad_input=torchvision.transforms.Pad(5,padding_mode='edge')\n",
        "        #initialize_weights([self.act], 0.00002)\n",
        "        self.counter = 0\n",
        "    def forward(self,input):\n",
        "        self.counter+=1\n",
        "        u,f=input\n",
        "        up = self.pad_input(u)\n",
        "        ur = up.repeat(1,self.in_channels,1,1)\n",
        "        #\n",
        "        output1 = F.conv2d(ur, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
        "        output,_ = self.act(output1)\n",
        "        weight_rot180 = torch.rot90(torch.rot90(self.weight, 1, [2, 3]),1,[2,3])\n",
        "        #\n",
        "        output = F.conv2d(output, weight_rot180, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
        "        output = F.pad(output,(-5,-5,-5,-5))\n",
        "        #import pdb; pdb.set_trace()\n",
        "        if self.counter%100==0:\n",
        "            print(self.alpha,self.beta.max(),self.beta.min(),self.act.weight.max(),self.act.weight.min(),output.sum(1,keepdim=True).max())\n",
        "        output = u-self.beta*output.sum(1,keepdim=True)-self.alpha*(u-f)\n",
        "       #u = u-output.mul(beta).sum(1,keepdim=True)-self.alpha.exp()*(u-f)\n",
        "       #u = (u-output.mul(beta).sum(1,keepdim=True)+math.sqrt({u-(output.mul(beta).sum(1,keepdim=True))}**2 + 8(1+2*self.alpha.exp()) * self.alpha.exp() * (f)**2))/(2 * (1+2*self.alpha.exp()))\n",
        "\n",
        "        return output,f\n",
        "\n",
        "class TNRDlayer(nn.Module):\n",
        "    \"\"\"docstring for TNRDConv2d.\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=5,\n",
        "                 stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
        "        super(TNRDlayer, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.stride=stride\n",
        "        self.groups=groups\n",
        "        self.bias=bias\n",
        "        self.padding=padding\n",
        "        self.stride=stride\n",
        "        self.dilation=dilation\n",
        "        self.kernel_size=kernel_size\n",
        "        self.act = RBF(_NRBF,self.in_channels,gaussian)\n",
        "        self.alpha=nn.Parameter(torch.Tensor([0.9]))\n",
        "        self.beta=nn.Parameter(torch.zeros([1,self.in_channels,1,1]))\n",
        "        if _DCT:\n",
        "            self.weight = nn.Parameter(torch.zeros([in_channels,in_channels]))\n",
        "        else:\n",
        "            self.weight = nn.Parameter(torch.zeros([in_channels,1,kernel_size,kernel_size]))\n",
        "            if not _TIE:\n",
        "                self.weight2 = nn.Parameter(torch.zeros([in_channels,1,kernel_size,kernel_size]))\n",
        "        if _C1x1:\n",
        "            self.weight_1x1=nn.Parameter(torch.zeros([in_channels,in_channels,1,1]))\n",
        "\n",
        "        #initialize_weights_dct([self], 0.02)\n",
        "        self.register_buffer('dct_filters',torch.tensor(gen_dct2(kernel_size)[1:,:]).float())\n",
        "\n",
        "        self.pad_input=torchvision.transforms.Pad(12,padding_mode='symmetric')\n",
        "        #initialize_weights([self.act], 0.00002)\n",
        "        self.counter = 0\n",
        "\n",
        "    def forward(self,input):\n",
        "        self.counter+=1\n",
        "        u,f=input\n",
        "\n",
        "        for it in range(1):\n",
        "            up = self.pad_input(u)\n",
        "            ur = up.repeat(1,self.in_channels,1,1)\n",
        "            #import pdb; pdb.set_trace()\n",
        "            if _DCT:\n",
        "                K=self.weight.matmul(self.dct_filters)\n",
        "                K = K.div(torch.norm(K,dim=1,keepdim=True)+2.2e-16).view(self.kernel_size**2-1,1,self.kernel_size,self.kernel_size)\n",
        "            else:\n",
        "                K = self.weight\n",
        "\n",
        "            output1 = F.conv2d(ur, K, None, self.stride, self.padding, self.dilation, self.groups)\n",
        "            output,_ = self.act(output1)\n",
        "            weight_rot180 = torch.rot90(torch.rot90(K, 1, [2, 3]),1,[2,3]) if _TIE else self.weight2\n",
        "            if _C1x1:\n",
        "                output = F.conv2d(output, self.weight_1x1, None, self.stride, self.padding, self.dilation)\n",
        "            output = F.conv2d(output, weight_rot180, None, self.stride, self.padding, self.dilation, self.groups)\n",
        "            output = F.pad(output,(-8,-8,-8,-8))\n",
        "            #\n",
        "            if self.counter%500==0:\n",
        "                print(self.alpha,self.beta.max(),self.beta.min(),self.act.weight.max(),self.act.weight.min(),output.sum(1,keepdim=True).max())\n",
        "            beta = self.beta if _BETA else 1\n",
        "            #u = u-output.mul(beta).sum(1,keepdim=True)-self.alpha.exp()*(u-f)\n",
        "            #u = (u-output.mul(beta).sum(1,keepdim=True)+math.sqrt((u-(output.mul(beta).sum(1,keepdim=True)))**2 + 8(1+2*self.alpha.exp()) * self.alpha.exp() * (f)**2))/(2 * (1+2*self.alpha.exp()))\n",
        "            ##############MODIFIED_HERE____------------\n",
        "            u = (u - output.mul(beta).sum(1, keepdim=True) + torch.sqrt((u - output.mul(beta).sum(1, keepdim=True))**2 + 8 * (1 + 2 * self.alpha.exp()) * self.alpha.exp() * (f)**2)) / (2 * (1 + 2 * self.alpha.exp()))\n",
        "        return u,f\n",
        "class GenBlock(nn.Module):\n",
        "    def __init__(self, in_channels=64, out_channels=64, kernel_size=5, bias=True):\n",
        "        super(GenBlock, self).__init__()\n",
        "        self.conv = TNRDConv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=(kernel_size // 2), bias=bias)\n",
        "        self.bn = nn.BatchNorm2d(num_features=out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        initialize_weights([self.conv, self.bn], 0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn(self.conv(x)))\n",
        "        return x\n",
        "\n",
        "class DisBlock(nn.Module):\n",
        "    def __init__(self, in_channels=64, out_channels=64, bias=True, normalization=False):\n",
        "        super(DisBlock, self).__init__()\n",
        "        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1, bias=bias)\n",
        "        self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=4, stride=2, padding=1, bias=bias)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels, affine=True)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels, affine=True)\n",
        "\n",
        "        initialize_weights([self.conv1, self.conv2], 0.1)\n",
        "\n",
        "        if normalization:\n",
        "            self.conv1 = SpectralNorm(self.conv1)\n",
        "            self.conv2 = SpectralNorm(self.conv2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lrelu(self.bn1(self.conv1(x)))\n",
        "        x = self.lrelu(self.bn2(self.conv2(x)))\n",
        "        return x\n",
        "\n",
        "class Generatorv1(nn.Module):\n",
        "    def __init__(self, in_channels, num_features, gen_blocks, dis_blocks):\n",
        "        super(Generatorv1, self).__init__()\n",
        "        filter_size=5\n",
        "        # image to features\n",
        "        in_channels=filter_size**2\n",
        "\n",
        "        #self.crop_output=torchvision.transforms.CenterCrop(50)\n",
        "        self.image_to_features = TNRDConv2d(in_channels=in_channels, out_channels=in_channels,kernel_size=filter_size, groups=in_channels)\n",
        "        # features\n",
        "        blocks = []\n",
        "        for _ in range(gen_blocks):\n",
        "            blocks.append(TNRDConv2d(in_channels=in_channels, out_channels=in_channels,kernel_size=filter_size, bias=False,groups=in_channels))\n",
        "        self.features = nn.Sequential(*blocks)\n",
        "\n",
        "        # features to image\n",
        "        self.features_to_image = TNRDConv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=filter_size,groups=in_channels)\n",
        "        initialize_weights([self.features_to_image], 0.02)\n",
        "        self.counter=0\n",
        "    def forward(self, x):\n",
        "        self.counter+=1\n",
        "        x = self.image_to_features([x,x])\n",
        "        x = self.features(x)\n",
        "        x = self.features_to_image(x)\n",
        "        return x[0]\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_channels, num_features, gen_blocks, dis_blocks):\n",
        "        super(Generator, self).__init__()\n",
        "        filter_size=5\n",
        "        # image to features\n",
        "        in_channels=(filter_size**2-1)\n",
        "\n",
        "        #self.crop_output=torchvision.transforms.CenterCrop(50)\n",
        "        self.image_to_features = TNRDlayer(in_channels=in_channels, out_channels=in_channels,groups=in_channels)\n",
        "        # features\n",
        "        blocks = []\n",
        "        for _ in range(gen_blocks):\n",
        "            blocks.append(TNRDlayer(in_channels=in_channels, out_channels=in_channels, bias=False,groups=in_channels))\n",
        "        self.features = nn.Sequential(*blocks)\n",
        "\n",
        "        #self.features = []\n",
        "        #for _ in range(gen_blocks):\n",
        "        #    self.features.append(TNRDlayer(in_channels=in_channels, out_channels=in_channels, bias=False,groups=in_channels))\n",
        "\n",
        "        # features to image\n",
        "        self.features_to_image = TNRDlayer(in_channels=in_channels, out_channels=in_channels, kernel_size=5,groups=in_channels)\n",
        "        #initialize_weights([self.features_to_image], 0.02)\n",
        "        self.counter=0\n",
        "        init_model_param(self,num_reb_kernels=_NRBF,filter_size=5,stage=gen_blocks+2)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        self.counter+=1\n",
        "        x = self.image_to_features([x,x])\n",
        "        x = self.features(x)\n",
        "        x = self.features_to_image(x)\n",
        "        return x[0]\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels, num_features, gen_blocks, dis_blocks):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        # image to features\n",
        "        self.image_to_features = DisBlock(in_channels=in_channels, out_channels=num_features, bias=True, normalization=False)\n",
        "\n",
        "        # features\n",
        "        blocks = []\n",
        "        for i in range(0, dis_blocks - 1):\n",
        "            blocks.append(DisBlock(in_channels=num_features * min(pow(2, i), 8), out_channels=num_features * min(pow(2, i + 1), 8), bias=False, normalization=False))\n",
        "        self.features = nn.Sequential(*blocks)\n",
        "\n",
        "        # classifier\n",
        "        self.classifier = nn.Conv2d(in_channels=num_features * min(pow(2, dis_blocks - 1), 8), out_channels=1, kernel_size=4, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.image_to_features(x)\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        x = x.flatten(start_dim=1).mean(dim=-1)\n",
        "        return x\n",
        "\n",
        "class SNDiscriminator(nn.Module):\n",
        "    def __init__(self, in_channels, num_features, gen_blocks, dis_blocks):\n",
        "        super(SNDiscriminator, self).__init__()\n",
        "\n",
        "        # image to features\n",
        "        self.image_to_features = DisBlock(in_channels=in_channels, out_channels=num_features, bias=True, normalization=True)\n",
        "\n",
        "        # features\n",
        "        blocks = []\n",
        "        for i in range(0, dis_blocks - 1):\n",
        "            blocks.append(DisBlock(in_channels=num_features * min(pow(2, i), 8), out_channels=num_features * min(pow(2, i + 1), 8), bias=False, normalization=True))\n",
        "        self.features = nn.Sequential(*blocks)\n",
        "\n",
        "        # classifier\n",
        "        self.classifier = SpectralNorm(nn.Conv2d(in_channels=num_features * min(pow(2, dis_blocks - 1), 8), out_channels=1, kernel_size=4, padding=0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.image_to_features(x)\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        x = x.flatten(start_dim=1).mean(dim=-1)\n",
        "        return x\n",
        "\n",
        "def g_tnrd(**config):\n",
        "    config.setdefault('in_channels', 3)\n",
        "    config.setdefault('num_features', 64)\n",
        "    config.setdefault('gen_blocks', 3)\n",
        "    config.setdefault('dis_blocks', 5)\n",
        "\n",
        "    return Generator(**config)\n",
        "\n",
        "def d_tnrd(**config):\n",
        "    config.setdefault('in_channels', 3)\n",
        "    config.setdefault('num_features', 64)\n",
        "    config.setdefault('gen_blocks', 8)\n",
        "    config.setdefault('dis_blocks', 5)\n",
        "\n",
        "    return Discriminator(**config)"
      ],
      "metadata": {
        "id": "bp4Bskvpuycf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch 2\n",
        "\n",
        "Epoch 3\n",
        "\n",
        "Epoch 4\n",
        "\n",
        "Epoch 5\n",
        "\n",
        "Epoch 6\n",
        "\n",
        "Epoch 7\n",
        "\n",
        "Epoch 8\n",
        "\n",
        "Epoch 9\n",
        "\n",
        "Epoch 10\n",
        "Iteration 101, G: 1850.02942, G_recon: 1850.029415, tnrd_loss: 0.000086,\n",
        "Evaluation: 16.957\n",
        "\n",
        "Epoch 11\n",
        "\n",
        "Epoch 12\n",
        "\n",
        "Epoch 13\n",
        "\n",
        "Epoch 14\n",
        "\n",
        "Epoch 15\n",
        "\n",
        "Epoch 16\n",
        "\n",
        "Epoch 17\n",
        "\n",
        "Epoch 18\n",
        "\n",
        "Epoch 19\n",
        "Iteration 201, G: 908.25291, G_recon: 908.252906, tnrd_loss: 0.000000,\n",
        "\n",
        "Epoch 20\n",
        "Evaluation: 22.181\n",
        "Best PSNR Score: 22.18\n",
        "█$ colab>>  "
      ],
      "metadata": {
        "id": "hSUkm9q8oy3h"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2UelgfO0o5zW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}